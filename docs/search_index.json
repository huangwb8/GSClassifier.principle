[["index.html", "The Principle of R Package GSClassifier Welcome About License Installation Mirror Change log TODO Other Projects", " The Principle of R Package GSClassifier Weibin Huang 2022-09-30 Welcome About The Priciple of GSClassifier is a book for users of the R package GSClassifier who want to know the most details. If you’re looking for the PDF edition, you can find it here. GSClassifier is an R-based comprehensive classification tool for subtypes modeling and personalized calling based on pure transcriptomics. It could be used for precision medicine, such as cancer diagnosis. The inspiration for GSClassifier comes from ImmuneSubtypeClassifier, an R package for classification of PanCancer immune subtypes based on the work of Gibbs et al [1,2]. Lots of surprising features in GSClassifier are as follows: Optimized for just one sample Available for modeling and calling of brand-new GEPs-based subtypes in any diseases (cancers) No limitation of the number of gene signatures(1) or subtypes(2) Insensitive normalization due to the use of the individual gene rank matrix More ensemble and repeatable modeling process More optimizations in the parallel computing New useful functions as supplements ATTENTION! In the future, there might be third-party contributors in GSClassifier platform, with some useful models for specific usages. If you use models provided by these people, you had better know more details as possible, including designs, data sources, destinations, training scripts, and limitations of models, especially those from studies under peer review. License GSClassifier is released under the Apache-2.0 license. See LICENSE for details. The technical documentation, as a whole, is licensed under a Creative Commons Attribution- NonCommercial-ShareAlike 4.0 International License. The code contained in this book is simultaneously available under the MIT license; this means that you are free to use it in your packages, as long as you cite the source. Installation RStudio/Posit is one of the best Integrated Development Environments (IDE) in R programming. If you’re struggling in R-GUI, it is recommended to turn to RStudio/Posit. For installation of GSClassifier, please run these commands in an R environment: # Install &quot;devtools&quot; package if (!requireNamespace(&quot;devtools&quot;, quietly = TRUE)) install.packages(&quot;devtools&quot;) # Install dependencies if (!requireNamespace(&quot;luckyBase&quot;, quietly = TRUE)) devtools::install_github(&quot;huangwb8/luckyBase&quot;) # Install the &quot;GSClassifier&quot; package if (!requireNamespace(&quot;GSClassifier&quot;, quietly = TRUE)) devtools::install_github(&quot;huangwb8/GSClassifier&quot;) In the future, a stable GSClassifier version might be sent to CRAN. Still beta. Mirror For some special countries or regions, users could also try: # Install dependencies install.packages(&quot;https://gitee.com/huangwb8/luckyBase/repository/archive/Primary?format=tar.gz&quot;, repos=NULL, method=&quot;libcurl&quot;) # Install the &quot;GSClassifier&quot; package install.packages(&quot;https://gitee.com/huangwb8/GSClassifier/repository/archive/Primary?format=tar.gz&quot;, repos=NULL, method=&quot;libcurl&quot;) Change log Version 0.1.9 Optimize function verbose Optimize for a routine scenario: one gene set and two subtypes Optimize the strategy of automatic parameters selection for modeling training with R package caret Interact with external models from the luckyModel package Version 0.1.8 Primary public version of GSClassifier Apache License, Version 2.0 Friendly wiki-based tutorial Platform for developers TODO More medical fields included, such as in the pan-cancer utility Advanced methods (such as artificial intelligence) for enhanced robustness Unsupervised learning for de-novo classification based on intrinsic frames of omics instead of human knowledge Multi-omics exploration and support More friendly characteristics for developers and contributors Web application for newbies to R programming Other Projects You may also be interested in: “luckyBase” The base functions of lucky series. “luckyModel” Model ensemble for third-party lucky series, such GSClassifier. "],["the-principle-of-gsclassifier.html", "Chapter 1 The Principle of GSClassifier 1.1 Packages 1.2 Flowchart 1.3 Top scoring pairs", " Chapter 1 The Principle of GSClassifier Leave some introductions 1.1 Packages # Install &quot;devtools&quot; package if (!requireNamespace(&quot;devtools&quot;, quietly = TRUE)) install.packages(&quot;devtools&quot;) # Install dependencies if (!requireNamespace(&quot;luckyBase&quot;, quietly = TRUE)) devtools::install_github(&quot;huangwb8/luckyBase&quot;) # Install the &quot;GSClassifier&quot; package if (!requireNamespace(&quot;GSClassifier&quot;, quietly = TRUE)) devtools::install_github(&quot;huangwb8/GSClassifier&quot;) # Install the &quot;pacman&quot; package if (!requireNamespace(&quot;pacman&quot;, quietly = TRUE)){ install.packages(&quot;pacman&quot;) library(pacman) } else { library(pacman) } # Load needed packages packages_needed &lt;- c( &quot;readxl&quot;, &quot;ComplexHeatmap&quot;, &quot;GSClassifier&quot;, &quot;rpart&quot;, &quot;tidyr&quot;, &quot;reshape2&quot;, &quot;ggplot2&quot;) for(i in packages_needed){p_load(char=i)} Here is the environment of R programming: # R version 4.0.3 (2020-10-10) # Platform: x86_64-w64-mingw32/x64 (64-bit) # Running under: Windows 10 x64 (build 18363) # # Matrix products: default # # locale: # [1] LC_COLLATE=Chinese (Simplified)_China.936 # [2] LC_CTYPE=Chinese (Simplified)_China.936 # [3] LC_MONETARY=Chinese (Simplified)_China.936 # [4] LC_NUMERIC=C # [5] LC_TIME=Chinese (Simplified)_China.936 # # attached base packages: # [1] grid stats graphics grDevices utils datasets methods # [8] base # # other attached packages: # [1] ggplot2_3.3.6 reshape2_1.4.4 tidyr_1.2.0 # [4] rpart_4.1.16 GSClassifier_0.1.25 luckyBase_0.1.0 # [7] ComplexHeatmap_2.4.3 readxl_1.4.0 pacman_0.5.1 # # loaded via a namespace (and not attached): # [1] colorspace_2.0-3 ggsignif_0.6.3 rjson_0.2.21 # [4] ellipsis_0.3.2 class_7.3-20 rprojroot_2.0.3 # [7] circlize_0.4.15 GlobalOptions_0.1.2 fs_1.5.2 # [10] clue_0.3-57 rstudioapi_0.13 listenv_0.8.0 # [13] ggpubr_0.4.0 remotes_2.4.2 lubridate_1.8.0 # [16] prodlim_2019.11.13 fansi_1.0.3 codetools_0.2-18 # [19] splines_4.0.3 doParallel_1.0.17 cachem_1.0.6 # [22] knitr_1.30 pkgload_1.2.4 jsonlite_1.8.0 # [25] pROC_1.18.0 caret_6.0-92 broom_1.0.0 # [28] cluster_2.1.3 png_0.1-7 compiler_4.0.3 # [31] backports_1.4.1 assertthat_0.2.1 Matrix_1.4-1 # [34] fastmap_1.1.0 cli_3.3.0 htmltools_0.5.2 # [37] prettyunits_1.1.1 tools_4.0.3 gtable_0.3.0 # [40] glue_1.6.2 dplyr_1.0.9 Rcpp_1.0.8.3 # [43] carData_3.0-5 cellranger_1.1.0 jquerylib_0.1.4 # [46] vctrs_0.4.1 nlme_3.1-149 iterators_1.0.14 # [49] timeDate_3043.102 xfun_0.33 gower_1.0.0 # [52] stringr_1.4.0 globals_0.15.1 ps_1.4.0 # [55] testthat_3.1.0 lifecycle_1.0.1 devtools_2.4.3 # [58] rstatix_0.7.0 future_1.26.1 MASS_7.3-53 # [61] scales_1.2.0 ipred_0.9-12 parallel_4.0.3 # [64] RColorBrewer_1.1-3 yaml_2.3.5 memoise_2.0.1 # [67] sass_0.4.1 stringi_1.7.6 desc_1.4.1 # [70] randomForest_4.6-14 foreach_1.5.2 hardhat_1.1.0 # [73] pkgbuild_1.3.1 lava_1.6.10 shape_1.4.6 # [76] tuneR_1.4.0 rlang_1.0.2 pkgconfig_2.0.3 # [79] evaluate_0.15 lattice_0.20-41 purrr_0.3.4 # [82] recipes_0.2.0 processx_3.7.0 tidyselect_1.1.2 # [85] parallelly_1.32.0 plyr_1.8.7 magrittr_2.0.3 # [88] bookdown_0.21 R6_2.5.1 generics_0.1.2 # [91] DBI_1.1.3 pillar_1.7.0 withr_2.5.0 # [94] survival_3.3-1 abind_1.4-5 nnet_7.3-17 # [97] tibble_3.1.7 future.apply_1.9.0 crayon_1.5.1 # [100] car_3.1-0 xgboost_1.6.0.1 utf8_1.2.2 # [103] rmarkdown_2.14 GetoptLong_1.0.5 usethis_2.1.3 # [106] data.table_1.14.2 callr_3.7.0 ModelMetrics_1.2.2.2 # [109] digest_0.6.29 stats4_4.0.3 signal_0.7-7 # [112] munsell_0.5.0 bslib_0.3.1 sessioninfo_1.2.2 1.2 Flowchart The flowchart of GSClassifier is showed in Figure 1.1. Figure 1.1: The flow chart of GSClassifier 1.2.1 Data Processing For each dataset, the RNA expression matrix would be normalized internally (Raw Matrix) so that the expression data of the samples in the dataset were comparable and suitable for subtype identification. As demonstrated in Figure 1.1, the Subtype Vector is identified based on independent cohorts instead of a merged matrix with batch effect control technologies. More details about batch effect control are discussed in 2.3. There is no standard method to figure out subtype vectors. It depends on the Gene Expression Profiles (GEPs), the biological significance, or the ideas of researchers. For Pan-immune Activation and Dysfunction (PAD) subtypes, the GEPs, Pan-Immune Activation Module (PIAM) and Pan-Immune Dysfunction Genes (PIDG), are biologically associated and suitable for calling four subtypes (PIAMhighPIDGhigh, PIAMhighPIDGlow, PIAMlowPIDGhigh, and PIAMlowPIDGlow). Theoretically, we can also use a category strategy like low/medium/high, but more evidence or motivations are lacking for chasing such a complex model. With subtype vectors and raw matrices, Top Scoring Pairs (TSP), the core data format for model training and application in GSClassifier, would be calculated for the following process. The details of TSP normalization are summarized in 1.3. 1.2.2 Model Establishment and Validation The TSP matrix would be divided into the training cohort and the internal validation cohort. In the PAD project, the rate of samples (training vs. test) is 7:3. Next, each SubSet (70% of the training cohort) would be further selected randomly to build a SubModel via cross-validation Extreme Gradient Boosting algorithm (xgboost::xgb.cv function) [3]. The number of submodels is suggested over 20 (more details in 4.5). The internal validation cohort and external validation cohort (if any) would be used to test the performance of the trained model. By the way, the data of both internal and external validation cohorts would not be used during model training to avoid over-fitting. 1.2.3 Model Application In the PAD project, Model for individual, the ensemble of submodels, is called “PAD for individual” (PADi). Supposed raw RNA expression of a sample was given. As shown in 1.1 and 1.2, GSClassifier would turn raw RNA expression into a TSP vector, which would be as an input to Model for individual. Then, GSClassifier would output the possibility matrix and the subtype for this sample. No extra data (RNA expression of others, follow-up data, etc) would be needed but RNA expression of the patient for subtype identification, so we suggest Model for individual (PADi, etc) as personalized model. 1.3 Top scoring pairs 1.3.1 Introduction Genes expression of an individual is normalized during the model training and the subtype identification via Top Scoring Pairs (TSP, also called Relative Expression Orderings (REOs)) algorithm, which was previously described by Geman et al [4]. TSP normalization for an individual depends on its transcript data, implying that subtype calling would not be perturbed by data from other individuals or other extra information like follow-up data. TSP had been used in cancer research and effectively predicts cancer progression and ICIs response [5–7]. As shown in Figure 1.2, The TSP data in GSClassifier consists of three parts: binned expression, pair difference, and set difference. In this section, we would conduct some experiments to demonstrate the potential of TSP normalization for development of cross-dataset/platform GEP-based models. Figure 1.2: The components of TSP (2 gene sets) 1.3.2 Simulated Dataset We simulated a dataset to demonstrate TSP normalization in GSClassifier: # Geneset geneSet &lt;- list( Set1 = paste(&#39;Gene&#39;,1:3,sep = &#39;&#39;), Set2 = paste(&#39;Gene&#39;,4:6,sep = &#39;&#39;) ) # RNA expression x &lt;- read_xlsx(&#39;./data/simulated-data.xlsx&#39;, sheet = &#39;RNA&#39;) expr0 &lt;- as.matrix(x[,-1]) rownames(expr0) &lt;- as.character(as.matrix(x[,1])); rm(x) # Missing value imputation (MVI) expr &lt;- na_fill(expr0, method = &quot;quantile&quot;, seed = 447) # Missing value imputation with quantile algorithm! # Subtype information # It depends on the application scenarios of GEPs subtype_vector &lt;- c(1, 1, 1, 2, 2, 2) # Binned data for subtype 1 Ybin &lt;- ifelse(subtype_vector == 1, 1, 0) # Parameters breakVec = c(0, 0.25, 0.5, 0.75, 1.0) # Report cat(c(&#39;\\n&#39;, &#39;Gene sets:&#39;, &#39;\\n&#39;)) print(geneSet) cat(&#39;RNA expression:&#39;, &#39;\\n&#39;) print(expr0); cat(&#39;\\n&#39;) cat(&#39;RNA expression after MVI:&#39;, &#39;\\n&#39;) print(expr) # # Gene sets: # $Set1 # [1] &quot;Gene1&quot; &quot;Gene2&quot; &quot;Gene3&quot; # # $Set2 # [1] &quot;Gene4&quot; &quot;Gene5&quot; &quot;Gene6&quot; # # RNA expression: # Sample1 Sample2 Sample3 Sample4 Sample5 Sample6 # Gene1 0.51 0.52 0.60 0.21 0.30 0.40 # Gene2 0.52 0.54 0.58 0.22 0.31 0.35 # Gene3 0.53 0.60 0.61 NA 0.29 0.30 # Gene4 0.21 0.30 0.40 0.51 0.52 0.60 # Gene5 0.22 0.31 0.35 0.52 0.54 0.58 # Gene6 0.23 0.29 0.30 0.53 NA 0.61 # Gene7 0.10 0.12 0.09 0.11 0.12 0.14 # # RNA expression after MVI: # Sample1 Sample2 Sample3 Sample4 Sample5 Sample6 # Gene1 0.51 0.52 0.60 0.2100 0.30000 0.40 # Gene2 0.52 0.54 0.58 0.2200 0.31000 0.35 # Gene3 0.53 0.60 0.61 0.2486 0.29000 0.30 # Gene4 0.21 0.30 0.40 0.5100 0.52000 0.60 # Gene5 0.22 0.31 0.35 0.5200 0.54000 0.58 # Gene6 0.23 0.29 0.30 0.5300 0.51774 0.61 # Gene7 0.10 0.12 0.09 0.1100 0.12000 0.14 Look at the matrix via heatmap: Heatmap(t(scale(t(expr0))), name = &quot;Z-score&quot;, column_title = &quot;Before MVI&quot;) Heatmap(t(scale(t(expr))), name = &quot;Z-score&quot;, column_title = &quot;After MVI&quot;) This is an interesting dataset with features as follows: Distinguished gene sets: The expression profile between Gene 1-3 and Gene 4-6 is different across samples. Thus, these gene sets might represent different biological significance. Stable gene: The expression level and rank of Gene 7 seemed to be similar across samples. Thus, Gene 7 might not be a robust marker for subtype modeling. Thus, it could help us to understand how the filtering of GSClassifier works. Expression heterogeneity &amp; rank homogeneity: Take Sample1 and Sample3 as examples. The expression of Gene 1-6 in Sample3 seemed to be higher than those of Sample1. However, the expression of Gene 1-3 is higher than Gene 4-6 in both Sample1 and Sample3, indicating similar bioprocess in these samples exists so that they should be classified as the same subtype. 1.3.3 Binned expression First, we binned genes with different quantile intervals so that the distribution of rank information could be more consistent across samples. Take Sample4 as an example: # Data of Sample4 x &lt;- expr[,4] # Create quantiles brks &lt;- quantile(as.numeric(x), probs=breakVec, na.rm = T) # Get interval orders xbin &lt;- .bincode(x = x, breaks = brks, include.lowest = T) xbin &lt;- as.numeric(xbin) names(xbin) &lt;- names(x) # Report cat(&#39;Quantiles:&#39;, &#39;\\n&#39;); print(brks) cat(&#39;\\n&#39;) cat(&#39;Raw expression:&#39;, &#39;\\n&#39;);print(x) cat(&#39;\\n&#39;) cat(&#39;Binned expression:&#39;, &#39;\\n&#39;); print(xbin) # Quantiles: # 0% 25% 50% 75% 100% # 0.1100 0.2150 0.2486 0.5150 0.5300 # # Raw expression: # Gene1 Gene2 Gene3 Gene4 Gene5 Gene6 Gene7 # 0.2100 0.2200 0.2486 0.5100 0.5200 0.5300 0.1100 # # Binned expression: # Gene1 Gene2 Gene3 Gene4 Gene5 Gene6 Gene7 # 1 2 2 3 4 4 1 For example, 0.110 is the minimun of the raw expression vector, so its binned expression is 1. Similarly, the binned expression of maximum 0.530 is 4. Generally, we calculate binned expression via function breakBin of GSClassifier: expr_binned &lt;- apply( expr, 2, GSClassifier:::breakBin, breakVec) rownames(expr_binned) &lt;- rownames(expr) print(expr_binned) # Sample1 Sample2 Sample3 Sample4 Sample5 Sample6 # Gene1 3 3 4 1 2 2 # Gene2 4 4 3 2 2 2 # Gene3 4 4 4 2 1 1 # Gene4 1 2 2 3 4 4 # Gene5 2 2 2 4 4 3 # Gene6 2 1 1 4 3 4 # Gene7 1 1 1 1 1 1 In this simulated dataset, Gene7 is a gene whose expression is always the lowest across all samples. In other words, the rank of Gene7 is stable or invariable across samples so it’s not robust for the identification of differential subtypes. Except for binned expression, we also calculated pair difference later. Because the number of gene pairs is \\(C_{2 \\atop n}\\), the exclusion of genes like Gene7 before modeling could reduce the complexity and save computing resources. In all, genes with low-rank differences should be dropped out to some extent in GSClassifier. First, We use base::rank to return the sample ranks of the values in a vector: expr_binned_rank &lt;- apply( expr_binned, 2, function(x)rank(x, na.last = TRUE) ) print(expr_binned_rank) # Sample1 Sample2 Sample3 Sample4 Sample5 Sample6 # Gene1 5.0 5.0 6.5 1.5 3.5 3.5 # Gene2 6.5 6.5 5.0 3.5 3.5 3.5 # Gene3 6.5 6.5 6.5 3.5 1.5 1.5 # Gene4 1.5 3.5 3.5 5.0 6.5 6.5 # Gene5 3.5 3.5 3.5 6.5 6.5 5.0 # Gene6 3.5 1.5 1.5 6.5 5.0 6.5 # Gene7 1.5 1.5 1.5 1.5 1.5 1.5 Then, get weighted average rank difference of each gene based on specified subtype distribution (Ybin): testRes &lt;- sapply( 1:nrow(expr_binned_rank), function(gi){ # Rank vector of each gene rankg = expr_binned_rank[gi,]; # Weighted average rank difference of a gene for specified subtype # Here is subtype 1 vs. others (sum(rankg[Ybin == 0], na.rm = T) / sum(Ybin == 0, na.rm = T)) - (sum(rankg[Ybin == 1], na.rm = T) / sum(Ybin == 1, na.rm = T)) } ) names(testRes) &lt;- rownames(expr_binned_rank) print(testRes) # Gene1 Gene2 Gene3 Gene4 Gene5 Gene6 Gene7 # -2.666667 -2.500000 -4.333333 3.166667 2.500000 3.833333 0.000000 Gene7 is the one with the lowest absolute value (0) of rank diffrence. By the way, Gene 1-3 have the same direction (&lt;0), and so does Gene 4-6 (&gt;0), which indicates the nature of clustering based on these two gene sets. In practice, we use ptail to select differential genes based on rank diffrences. Smaller ptail is, less gene kept. Here, we just set ptail=0.4: # ptail is a numeber ranging (0,0.5]. ptail = 0.4 # Index of target genes with big rank differences idx &lt;- which((testRes &lt; quantile(testRes, ptail, na.rm = T)) | (testRes &gt; quantile(testRes, 1.0-ptail, na.rm = T))) # Target genes gene_bigRank &lt;- names(testRes)[idx] # Report cat(&#39;Index of target genes: &#39;,&#39;\\n&#39;);print(idx); cat(&#39;\\n&#39;) cat(&#39;Target genes:&#39;,&#39;\\n&#39;);print(gene_bigRank) # Index of target genes: # Gene1 Gene2 Gene3 Gene4 Gene5 Gene6 # 1 2 3 4 5 6 # # Target genes: # [1] &quot;Gene1&quot; &quot;Gene2&quot; &quot;Gene3&quot; &quot;Gene4&quot; &quot;Gene5&quot; &quot;Gene6&quot; Hence, Gene7 was filtered and excluded in the following analysis. By the way, both ptail and breakVec are hyperparameters in GSClassifier modeling. 1.3.4 Pair difference In GSClassifier, we use an ensemble function featureSelection to select data for pair difference scoring. expr_feat &lt;- featureSelection(expr, Ybin, testRes = testRes, ptail = 0.4) expr_sub &lt;- expr_feat$Xsub gene_bigRank &lt;- expr_feat$Genes # Report cat(&#39;Raw xpression without NA:&#39;, &#39;\\n&#39;) print(expr_sub) cat(&#39;\\n&#39;) cat(&#39;Genes with large rank diff:&#39;, &#39;\\n&#39;) print(gene_bigRank) # Raw xpression without NA: # Sample1 Sample2 Sample3 Sample4 Sample5 Sample6 # Gene1 0.51 0.52 0.60 0.2100 0.30000 0.40 # Gene2 0.52 0.54 0.58 0.2200 0.31000 0.35 # Gene3 0.53 0.60 0.61 0.2486 0.29000 0.30 # Gene4 0.21 0.30 0.40 0.5100 0.52000 0.60 # Gene5 0.22 0.31 0.35 0.5200 0.54000 0.58 # Gene6 0.23 0.29 0.30 0.5300 0.51774 0.61 # # Genes with large rank diff: # [1] &quot;Gene1&quot; &quot;Gene2&quot; &quot;Gene3&quot; &quot;Gene4&quot; &quot;Gene5&quot; &quot;Gene6&quot; In GSClassifier, we use function makeGenePairs to calculate pair differences: gene_bigRank_pairs &lt;- GSClassifier:::makeGenePairs( gene_bigRank, expr[gene_bigRank,]) print(gene_bigRank_pairs) # Sample1 Sample2 Sample3 Sample4 Sample5 Sample6 # Gene1:Gene2 0 0 1 0 0 1 # Gene1:Gene3 0 0 0 0 1 1 # Gene1:Gene4 1 1 1 0 0 0 # Gene1:Gene5 1 1 1 0 0 0 # Gene1:Gene6 1 1 1 0 0 0 # Gene2:Gene3 0 0 0 0 1 1 # Gene2:Gene4 1 1 1 0 0 0 # Gene2:Gene5 1 1 1 0 0 0 # Gene2:Gene6 1 1 1 0 0 0 # Gene3:Gene4 1 1 1 0 0 0 # Gene3:Gene5 1 1 1 0 0 0 # Gene3:Gene6 1 1 1 0 0 0 # Gene4:Gene5 0 0 1 0 0 1 # Gene4:Gene6 0 1 1 0 1 0 # Gene5:Gene6 0 1 1 0 1 0 Take Gene1:Gene4 of Sample1 as an example. \\(Expression_{Gene1} - Expression_{Gene4} = 0.51-0.21 = 0.3 &gt; 0\\), so the pair score is 1. If the difference is less than or equal to 0, the pair score is 0. In addition, the scoring differences of gene pairs between Sample 1-3 and Sample 4-6 are obvious, revealing the robustness of pair difference for subtype identification. 1.3.5 Set difference In GSClassifier, set difference is defined as a weight average of gene-geneset rank difference. # No. of gene sets nGS = 2 # Name of gene set comparision, which is like s1s2, s1s3 and so on. featureNames &lt;- &#39;s1s2&#39; # Gene set difference across samples resultList &lt;- list() for (i in 1:ncol(expr_sub)) { # i=1 res0 &lt;- numeric(length=length(featureNames)) idx &lt;- 1 for (j1 in 1:(nGS-1)) { # j1=1 for (j2 in (j1+1):nGS) { # j2=2 # If j1=1 and j2=2, gene sets s1/s2 would be selected # Genes of different gene sets set1 &lt;- geneSet[[j1]] # &quot;Gene1&quot; &quot;Gene2&quot; &quot;Gene3&quot; set2 &lt;- geneSet[[j2]] # &quot;Gene4&quot; &quot;Gene5&quot; &quot;Gene6&quot; # RNA expression of Genes by different gene sets vals1 &lt;- expr_sub[rownames(expr_sub) %in% set1,i] # Gene1 Gene2 Gene3 # 0.51 0.52 0.53 vals2 &lt;- expr_sub[rownames(expr_sub) %in% set2,i] # Gene4 Gene5 Gene6 # 0.21 0.22 0.23 # Differences between one gene and gene sets # Compare expression of each gene in Set1 with all genes in Set2. # For example, 0.51&gt;0.21/0.22/0.23, so the value of Gene1:s2 is 3. res1 &lt;- sapply(vals1, function(v1) sum(v1 &gt; vals2, na.rm=T)) # Gene1:s2 Gene2:s2 Gene3:s2 # 3 3 3 # Weight average of gene-geneset rank difference res0[idx] &lt;- sum(res1, na.rm = T) / (length(vals1) * length(vals2)) # Next gene set pair idx &lt;- idx + 1 } } resultList[[i]] &lt;- as.numeric(res0) } resMat &lt;- do.call(cbind, resultList) colnames(resMat) &lt;- colnames(expr_sub) rownames(resMat) &lt;- featureNames # Report cat(&#39;Set difference across samples: &#39;, &#39;\\n&#39;) print(resMat) # Set difference across samples: # Sample1 Sample2 Sample3 Sample4 Sample5 Sample6 # s1s2 1 1 1 0 0 0 In GSClassifier, we established makeSetData to evaluate set difference across samples: # Gene set difference across samples geneset_interaction &lt;- GSClassifier:::makeSetData(expr_sub, geneSet) # Report cat(&#39;Set difference across samples: &#39;, &#39;\\n&#39;) print(resMat) # Set difference across samples: # Sample1 Sample2 Sample3 Sample4 Sample5 Sample6 # s1s2 1 1 1 0 0 0 We have known that the subtype of Sample 1-3 differs from that of Sample 4-6, which revealed the robustness of set differences for subtype identification. Based on the structure of TSP in Figure 1.2, the TSP matrix of the simulated dataset should be : # TSP matrix tsp &lt;- rbind( # Binned expression expr_binned[gene_bigRank,], # Pair difference gene_bigRank_pairs, # Set difference resMat ) # Report cat(&#39;TSP matrix: &#39;, &#39;\\n&#39;) print(tsp) # TSP matrix: # Sample1 Sample2 Sample3 Sample4 Sample5 Sample6 # Gene1 3 3 4 1 2 2 # Gene2 4 4 3 2 2 2 # Gene3 4 4 4 2 1 1 # Gene4 1 2 2 3 4 4 # Gene5 2 2 2 4 4 3 # Gene6 2 1 1 4 3 4 # Gene1:Gene2 0 0 1 0 0 1 # Gene1:Gene3 0 0 0 0 1 1 # Gene1:Gene4 1 1 1 0 0 0 # Gene1:Gene5 1 1 1 0 0 0 # Gene1:Gene6 1 1 1 0 0 0 # Gene2:Gene3 0 0 0 0 1 1 # Gene2:Gene4 1 1 1 0 0 0 # Gene2:Gene5 1 1 1 0 0 0 # Gene2:Gene6 1 1 1 0 0 0 # Gene3:Gene4 1 1 1 0 0 0 # Gene3:Gene5 1 1 1 0 0 0 # Gene3:Gene6 1 1 1 0 0 0 # Gene4:Gene5 0 0 1 0 0 1 # Gene4:Gene6 0 1 1 0 1 0 # Gene5:Gene6 0 1 1 0 1 0 # s1s2 1 1 1 0 0 0 Have a look at the distribution: # Data tsp_df &lt;- reshape2::melt(tsp) ggplot(tsp_df,aes(x=Var2,y=value,fill=Var2)) + geom_boxplot(outlier.size = 1, size = 1) + labs(x = &#39;Samples&#39;, y = &#39;Epression&#39;, fill = NULL) "],["discussion.html", "Chapter 2 Discussion 2.1 Packages 2.2 Missing value imputation (MVI) 2.3 Batch effect 2.4 Subtype number", " Chapter 2 Discussion In this section, we would discuss some key topics about GSClassifier, including Missing value imputation (MVI), Batch effect, hyperparameters, and so on. 2.1 Packages # Install &quot;devtools&quot; package if (!requireNamespace(&quot;devtools&quot;, quietly = TRUE)) install.packages(&quot;devtools&quot;) # Install dependencies if (!requireNamespace(&quot;luckyBase&quot;, quietly = TRUE)) devtools::install_github(&quot;huangwb8/luckyBase&quot;) # Install the &quot;**GSClassifier**&quot; package if (!requireNamespace(&quot;GSClassifier&quot;, quietly = TRUE)) devtools::install_github(&quot;huangwb8/GSClassifier&quot;) # Install the &quot;pacman&quot; package if (!requireNamespace(&quot;pacman&quot;, quietly = TRUE)){ install.packages(&quot;pacman&quot;) library(pacman) } else { library(pacman) } # Load needed packages packages_needed &lt;- c( &quot;readxl&quot;, &quot;ComplexHeatmap&quot;, &quot;GSClassifier&quot;, &quot;rpart&quot;, &quot;tidyr&quot;, &quot;reshape2&quot;, &quot;ggplot2&quot;) for(i in packages_needed){p_load(char=i)} Here is the environment of R programming: # R version 4.0.3 (2020-10-10) # Platform: x86_64-w64-mingw32/x64 (64-bit) # Running under: Windows 10 x64 (build 18363) # # Matrix products: default # # locale: # [1] LC_COLLATE=Chinese (Simplified)_China.936 # [2] LC_CTYPE=Chinese (Simplified)_China.936 # [3] LC_MONETARY=Chinese (Simplified)_China.936 # [4] LC_NUMERIC=C # [5] LC_TIME=Chinese (Simplified)_China.936 # # attached base packages: # [1] grid stats graphics grDevices utils datasets methods # [8] base # # other attached packages: # [1] ggplot2_3.3.6 reshape2_1.4.4 tidyr_1.2.0 # [4] rpart_4.1.16 GSClassifier_0.1.25 luckyBase_0.1.0 # [7] ComplexHeatmap_2.4.3 readxl_1.4.0 pacman_0.5.1 # # loaded via a namespace (and not attached): # [1] colorspace_2.0-3 ggsignif_0.6.3 rjson_0.2.21 # [4] ellipsis_0.3.2 class_7.3-20 rprojroot_2.0.3 # [7] circlize_0.4.15 GlobalOptions_0.1.2 fs_1.5.2 # [10] clue_0.3-57 rstudioapi_0.13 listenv_0.8.0 # [13] ggpubr_0.4.0 remotes_2.4.2 lubridate_1.8.0 # [16] prodlim_2019.11.13 fansi_1.0.3 codetools_0.2-18 # [19] splines_4.0.3 doParallel_1.0.17 cachem_1.0.6 # [22] knitr_1.30 pkgload_1.2.4 jsonlite_1.8.0 # [25] pROC_1.18.0 caret_6.0-92 broom_1.0.0 # [28] cluster_2.1.3 png_0.1-7 compiler_4.0.3 # [31] backports_1.4.1 assertthat_0.2.1 Matrix_1.4-1 # [34] fastmap_1.1.0 cli_3.3.0 htmltools_0.5.2 # [37] prettyunits_1.1.1 tools_4.0.3 gtable_0.3.0 # [40] glue_1.6.2 dplyr_1.0.9 Rcpp_1.0.8.3 # [43] carData_3.0-5 cellranger_1.1.0 jquerylib_0.1.4 # [46] vctrs_0.4.1 nlme_3.1-149 iterators_1.0.14 # [49] timeDate_3043.102 xfun_0.33 gower_1.0.0 # [52] stringr_1.4.0 globals_0.15.1 ps_1.4.0 # [55] testthat_3.1.0 lifecycle_1.0.1 devtools_2.4.3 # [58] rstatix_0.7.0 future_1.26.1 MASS_7.3-53 # [61] scales_1.2.0 ipred_0.9-12 parallel_4.0.3 # [64] RColorBrewer_1.1-3 yaml_2.3.5 memoise_2.0.1 # [67] sass_0.4.1 stringi_1.7.6 desc_1.4.1 # [70] randomForest_4.6-14 foreach_1.5.2 hardhat_1.1.0 # [73] pkgbuild_1.3.1 lava_1.6.10 shape_1.4.6 # [76] tuneR_1.4.0 rlang_1.0.2 pkgconfig_2.0.3 # [79] evaluate_0.15 lattice_0.20-41 purrr_0.3.4 # [82] recipes_0.2.0 processx_3.7.0 tidyselect_1.1.2 # [85] parallelly_1.32.0 plyr_1.8.7 magrittr_2.0.3 # [88] bookdown_0.21 R6_2.5.1 generics_0.1.2 # [91] DBI_1.1.3 pillar_1.7.0 withr_2.5.0 # [94] survival_3.3-1 abind_1.4-5 nnet_7.3-17 # [97] tibble_3.1.7 future.apply_1.9.0 crayon_1.5.1 # [100] car_3.1-0 xgboost_1.6.0.1 utf8_1.2.2 # [103] rmarkdown_2.14 GetoptLong_1.0.5 usethis_2.1.3 # [106] data.table_1.14.2 callr_3.7.0 ModelMetrics_1.2.2.2 # [109] digest_0.6.29 stats4_4.0.3 signal_0.7-7 # [112] munsell_0.5.0 bslib_0.3.1 sessioninfo_1.2.2 2.2 Missing value imputation (MVI) Due to reasons like weak signal, contamination of microarray surfaces, inappropriate manual operations, insufficient resolution, or systematic errors during the laboratory process [8–10], missing value in high-input genetic data is common. Generally, tiny missing values could be just dealt with case deletion, while the biological discovery might be damaged when the missing rate tops 15% [11,12]. Currently, lots of methods, including statistic-based or machine learning-based methods (Figure 2.1), had been developed for missing value imputation (MVI) [12]. Wang et al [13] categorized MVI methods into simple (zeros or average), biology knowledge-, global learning-, local learning-, and hybrid-based methods. In order to satisfy the working conditions of xgboost [14] functions (xgb.train, xgboost, and xgb.cv) in GSClassifer, missing values in the expression matrix must be deleted or imputation. Figure 2.1: Missing value imputation methods reviewed by Hasan et al. In PAD project, several strategies were applied to reduce the impact of missing values as possible. First, both PIAM and PIDG in PAD project were curated GEPs that were not missing in over 80% of gastric cancer datasets. Here we showed the actual distribution of missing values across samples in gastric cancer datasets we used. # Data testData &lt;- readRDS( system.file(&quot;extdata&quot;, &quot;testData.rds&quot;, package = &quot;GSClassifier&quot;) ) expr_pad &lt;- testData$PanSTAD_expr_part # Missing value expr_pad_na &lt;- apply(expr_pad, 2, function(x) sum(is.na(x))/length(x)) expr_pad_na_df &lt;- data.frame( sample = names(expr_pad_na), prob = as.numeric(expr_pad_na), stringsAsFactors = F ) As shown in Figure 2.2, the percentage of all samples in gastric cancer datasets we used is lower than 8%. # ggplot ggplot(data = expr_pad_na_df, aes(x = sample, y = prob)) + geom_bar(stat = &#39;identity&#39;, color = mycolor[3]) + scale_y_continuous(labels=scales::percent) + labs(x = &#39;Samples in gastric cancer cohorts&#39;, y = &#39;Percentage of missing value&#39;) + theme_bw() + theme( axis.text.x = element_blank(), axis.ticks = element_blank(), axis.title = element_text(size = 15), axis.text = element_text(size = 12) ) Figure 2.2: The distribution of missing value across gastric cancer samples. Second, we did conduct some MVI strategies to deal with data before model training in GSClassifier. Due to the low missing rate of our experimental data, we just set missing values as zero during model training and subtype identification in the early version of PADi (PAD.train.v20200110). The model seemed to be robust in both the internal cohort and external cohorts, and greatly predicted the response to immune checkpoint inhibitors (ICIs) in advanced gastric cancer. In the latest version of PADi (PAD.train.v20220916), we designed the so-called quantile algorithm for random MVI during PADi model training, which also seemed to work well for PADi model training. Here, we demonstrated the principle of quantile algorithm in the simulated dataset: # Simulated data x &lt;- read_xlsx(&#39;./data/simulated-data.xlsx&#39;, sheet = &#39;RNA&#39;) expr0 &lt;- as.matrix(x[,-1]) rownames(expr0) &lt;- as.character(as.matrix(x[,1])); rm(x) # MVI with Quantile algorithm expr &lt;- expr0 na.pos &lt;- apply(expr,2,is.one.na) set.seed(478); seeds &lt;- sample(1:ncol(expr)*10, sum(na.pos), replace = F) tSample &lt;- names(na.pos)[na.pos] quantile_vector &lt;- (1:1000)/1000 for(i in 1:length(tSample)){ # i=1 sample.i &lt;- tSample[i] expr.i &lt;- expr[, sample.i] expr.i.max &lt;- max(expr.i, na.rm = T) expr.i.min &lt;- min(expr.i, na.rm = T) set.seed(seeds[i]); # Details of quantile algorithm expr.i[is.na(expr.i)] &lt;- expr.i.min + (expr.i.max-expr.i.min) * sample(quantile_vector, sum(is.na(expr.i)), replace = T) expr[, sample.i] &lt;- expr.i } # Report cat(&#39;RNA expression:&#39;, &#39;\\n&#39;) print(expr0) cat(&#39;\\n&#39;) cat(&#39;RNA expression without NA value:&#39;, &#39;\\n&#39;) print(expr) # RNA expression: # Sample1 Sample2 Sample3 Sample4 Sample5 Sample6 # Gene1 0.51 0.52 0.60 0.21 0.30 0.40 # Gene2 0.52 0.54 0.58 0.22 0.31 0.35 # Gene3 0.53 0.60 0.61 NA 0.29 0.30 # Gene4 0.21 0.30 0.40 0.51 0.52 0.60 # Gene5 0.22 0.31 0.35 0.52 0.54 0.58 # Gene6 0.23 0.29 0.30 0.53 NA 0.61 # Gene7 0.10 0.12 0.09 0.11 0.12 0.14 # # RNA expression without NA value: # Sample1 Sample2 Sample3 Sample4 Sample5 Sample6 # Gene1 0.51 0.52 0.60 0.21000 0.30000 0.40 # Gene2 0.52 0.54 0.58 0.22000 0.31000 0.35 # Gene3 0.53 0.60 0.61 0.43256 0.29000 0.30 # Gene4 0.21 0.30 0.40 0.51000 0.52000 0.60 # Gene5 0.22 0.31 0.35 0.52000 0.54000 0.58 # Gene6 0.23 0.29 0.30 0.53000 0.32622 0.61 # Gene7 0.10 0.12 0.09 0.11000 0.12000 0.14 Look at the new matrix via heatmap, where the clustering result is not significantly disturbed after MVI: Heatmap(t(scale(t(expr))), name = &quot;Z-score&quot;, column_title = &quot;After MVI&quot;) Because missing values might damage the integrity of biological information, we explored how much the number of missing values in one sample impacts subtype identification via PADi. The steps are as follows: (i) we used the “quantile” algorithm to do MVI in the internal validation cohort of gastric cancer; (ii) we randomly masked different proportions of genes as zero expression; (iii) we calculated the relative multi-ROC [15] (masked data vs. MVI data). In GSClassifier, we developed a function called mv_tolerance to complete the task. Load the internal validation cohort: # Internal validation cohort testData &lt;- readRDS( system.file(&quot;extdata&quot;, &quot;testData.rds&quot;, package = &quot;GSClassifier&quot;) ) expr_pad &lt;- testData$PanSTAD_expr_part modelInfo &lt;- modelData( design = testData$PanSTAD_phenotype_part, id.col = &quot;ID&quot;, variable = c(&quot;platform&quot;, &quot;PAD_subtype&quot;), Prop = 0.7, seed = 19871 ) validInform &lt;- modelInfo$Data$Valid expr_pad_innervalid &lt;- expr_pad[,validInform$ID] Missing value tolerance analysis: # Time-consuming mvt &lt;- mv_tolerance( X = expr_pad_innervalid, gene.loss = c(2, 4, 6, 8, 10, 12), levels = c(1, 2, 3, 4), model = &quot;PAD.train_20220916&quot;, seed = 487, verbose = T ) multi-ROC analysis: # Data mvt_auc &lt;- mvt$multiAUC mvt_auc_df &lt;- data.frame() for(i in 1:length(mvt_auc)){ # i=1 df.i &lt;- data.frame( x = as.integer(Fastextra(names(mvt_auc)[i], &#39;=&#39;, 2)), y = as.numeric(mvt_auc[[i]]$auc), stringsAsFactors = F ) mvt_auc_df &lt;- rbind(mvt_auc_df, df.i) } # Plot ggplot(mvt_auc_df, aes(x,y)) + geom_point() + scale_x_continuous(breaks = c(2, 4, 6, 8, 10, 12)) + stat_smooth(formula = y ~ x,method = &#39;glm&#39;) + labs(x = &#39;No. of missing value&#39;, y = &#39;Relative AUC in multi-ROC analysis&#39;) + theme( axis.title = element_text(size = 15), axis.text = element_text(size = 12) ) Figure 2.3: The association between the number of missing value and subtype identification performance. As shown in Figure 2.3, there is a linear negative correlation between the number of missing values (missing rate ranges from 6.25% to 37.5%) and the subtype identification performance of PADi model. One of the reasons might be that PIAM/PIDG were small GEPs, so little gene loss might significantly impact the performance of PADi. By the way, there is no missing value in PIAM/PIDG of the ‘Kim2018’ cohort, an external validation cohort for ICIs therapy response prediction via PADi. Nonetheless, we still used the zero strategy during subtype identification of PADi if any missing values exist, because randomization might make the result unstable, which is not suitable for clinical decision. In conclusion, the zero or “quantile” strategy could be applied for MVI before GSClassifier model training. However, missing values should be avoided as possible in subtype identification for missing values could damage the performance of GSClassifier models. Nonetheless, due to low-input GEPs used in PADi model (No. of Gene=32), it’s easy to avoid missing value in clinical practice. 2.3 Batch effect TSP was widely applied to control batch effects in transcriptomic data [16–23]. Still, we tested whether TSP is a robust method for batch effect control in real-world data. As demonstrated in Figure 2.4, the obvious batch effects across gastric cancer datasets were significantly reduced after TSP normalization. Figure 2.4: Batch effects across gastric cancer cohorts. All gene pairs were used because subtype vectors were not specified. Top: Raw expression of all genes across samples. Middle: Raw expression of PIAM and PIDG across samples. Bottom: TSP of PIAM and PIDG across samples. To confirm the association between gene counts in modeling and batch effect control via TSP normalization, we selected random genes with counts ranging from 4 to 80 for TSP matrix establishment. As shown in Figure 2.5, TSP normalization works greatly in different gene counts for batch effect control compared with the raw expression matrix. Figure 2.5: Batch effects of random genes across gastric cancer cohorts. All gene pairs were used because subtype vectors were not specified. Gene counts 4, 8, 20, 40, and 80 were detected. Data of set difference were not available because only one gene set were applied. 2.4 Subtype number For PAD subtypes, it’s easy to determine the subtype number as 4. First, PAD subtypes are identified via 2 simple GEPs. The binary status (high/low expression) of 2 GEPs consists of a 2×2 matrix and hence the subtype number is exactly 4. Second, four PAD subtypes displayed different genetic/epigenetic alterations and clinical features (survival and ICI response), indicating that it’s biologically meaningful to distinguish gastric cancer into 4 immune subtypes. Third, clinicians are familiar with four subtypes, for the subtype number of the classical TNM stage in clinical oncology is 4 (Stage I to IV). Also, there’s another more simple situation—determining the subtype number with only one GEP, where 2 (high/low) or 3 (low/moderate/high) deserve to be tried. However, with the number of GEPs increasing, the situation would become more complex. Regrettably, GSClassifier can not help determine what subtype number should be used; GSClassifier only promises a robust model no matter how many GEPs or subtypes. There’s no gold standard for subtype number selection, which is more like art instead of math. Nevertheless, there’re several suggestions we can follow for best practice. First, the R package “ConsensusClusterPlus” [24–27] can help figure out consensus clustering for transcriptomic data, which provides visualization (consensus matrices, consensus cumulative distribution function plot, delta area plot, tracking plot, and so on) for clustering quality control. Second, paying more attention to the biological problem usually gives extra or even crucial clues for the subtype number decision. "],["quick-start.html", "Chapter 3 Quick start 3.1 About 3.2 Package 3.3 Data 3.4 PAD 3.5 PADi 3.6 Use external models from luckyModel package 3.7 PanCancer immune subtypes", " Chapter 3 Quick start 3.1 About Although with bright prospects in Pan-disease analysis, GSClassifier was primarily developed for clinic-friendly immune subtypes of gastric cancer (GC). Currently, only PAD subtypes and PADi for GC were supported. We would try to support more cancer types in the future as possible. More details in Plans in the future section. Gibbs' PanCancer immune subtypes based on five gene signatures (485 genes) could also be called in GSClassifier, with a pre-trained model from the ImmuneSubtypeClassifier package. If you use their jobs, please cite these papers. Particularly, all normal tissues should be eliminated before subtype identificaiton for cancer research. 3.2 Package # Install &quot;devtools&quot; package if (!requireNamespace(&quot;devtools&quot;, quietly = TRUE)) install.packages(&quot;devtools&quot;) # Install dependencies if (!requireNamespace(&quot;luckyBase&quot;, quietly = TRUE)) devtools::install_github(&quot;huangwb8/luckyBase&quot;) # Install the &quot;GSClassifier&quot; package if (!requireNamespace(&quot;GSClassifier&quot;, quietly = TRUE)) devtools::install_github(&quot;huangwb8/GSClassifier&quot;) # Load needed packages library(GSClassifier) # Loading required package: luckyBase 3.3 Data To lower the learning cost of GSClassifier, we provide some test data: testData &lt;- readRDS(system.file(&quot;extdata&quot;, &quot;testData.rds&quot;, package = &quot;GSClassifier&quot;)) Explore the testData: names(testData) # [1] &quot;Kim2018_3&quot; &quot;PanSTAD_phenotype_part&quot; &quot;PanSTAD_expr_part&quot; 3.4 PAD 3.4.1 The work flow of PAD exploration The basic process of PAD exploration was summarized in Figure 3.1 Figure 3.1: The process of PAD subtypes establishment With WGCNA method and TIMER datasets, Pan-Immune Activation Module (PIAM) was identified as a GEP representing co-infiltration of immune cells in the tumor microenvironment. Functional analysis such as GSEA was done for the exploration of PIAM functions. With feature selection based on Boruta algorithm, missing value control (without missing value in over 80% of GC datasets we used), we retained 101 genes and named as “PIAM-hub”. Pan-Immune Dysfunction Genes (PIDG) were explored based on “PIAM-hub” via a strategy similar to the computational framework of the Tumor Immune Dysfunction and Exclusion (TIDE) database [20]. Finally, 13 PIDGs were selected for downstream analysis as they were further validated in 2 or more external GC cohorts. To further reduce PIAM-hub for downstream modeling, genes with Mean of Correlation with Eigengene (MCE) 0.8 were selected and termed as “PIAM-top” subset (n=19). “PIAM-top” and PIDG, two curated tiny GEPs, were applied to establish Pan-immune Activation and Dysfunction (PAD) subtypes (PAD-I, PIAMhighPIDGhigh; PAD-II, PIAMhighPIDGlow; PAD-III, PIAMlowPIDGhigh; and PAD-IV, PIAMlowPIDGlow) in independent GC cohorts. Molecular alteration and patient survival across PAD subtypes were analyzed to figure out its biological and clinical impact. Also, a GSClassifier model called “PAD for individual” (PADi) was established for personalized subtype identification for immune checkpoint inhibitors response prediction in GC (More details in Online Section/PDF). 3.4.2 Preparation of the test data Load phenotype data: design &lt;- testData$PanSTAD_phenotype_part table(design$Dataset) # # GSE13861 GSE13911 GSE15459 GSE22377 GSE26899 GSE26901 GSE29272 GSE51105 # 65 39 192 43 96 108 134 94 # GSE54129 GSE57303 GSE62254 GSE65801 GSE84437 TCGA-STAD # 111 70 300 32 433 372 Load target sample IDs in GSE54129 cohort: target_ID &lt;- design$ID[design$Dataset %in% &#39;GSE54129&#39;] expr &lt;- testData$PanSTAD_expr_part[,target_ID] head(expr[,1:10]) # GSM1308413 GSM1308414 GSM1308415 GSM1308416 GSM1308417 # ENSG00000122122 7.888349 7.623663 6.873493 6.961102 7.150572 # ENSG00000117091 7.051760 6.217445 5.651839 5.830996 5.908532 # ENSG00000163219 6.056472 5.681844 5.411533 5.652684 5.555147 # ENSG00000136167 9.322191 8.765794 8.502315 8.838166 8.845952 # ENSG00000005844 7.119594 6.023631 5.400999 6.172863 6.059838 # ENSG00000123338 7.204051 6.925328 6.259809 6.610681 6.595882 # GSM1308418 GSM1308419 GSM1308420 GSM1308421 GSM1308422 # ENSG00000122122 7.871423 6.953329 8.334037 6.764335 6.522554 # ENSG00000117091 6.526917 5.646446 6.617520 5.637693 5.742848 # ENSG00000163219 5.962885 5.361763 5.975842 5.330428 5.172705 # ENSG00000136167 9.366074 8.675718 9.118517 8.614068 8.114096 # ENSG00000005844 6.523530 6.129181 7.331588 5.547059 5.867118 # ENSG00000123338 6.699790 6.935390 7.050288 6.536710 6.200269 3.4.3 Unsupervised clustering res_pad &lt;- PAD( expr = expr, cluster.method = &quot;ward.D2&quot;, extra.annot = NULL, plot.title = &#39;PAD test&#39;, subtype = &quot;PAD.train_20220916&quot;, verbose = T ) # Use default PIAM... # Use default PIDG... # Gene match: 100%. # Done! 3.4.4 Of note It’s strongly recommended that the gene type of expr should be always the same, such as ENSEMBL genes (ENSG00000111640 for GAPDH, for example). PAD function is only for datasets with lots of samples for its classification depends on population-based unsupervised clustering. PAD is population-dependent and non-personalized. Beta characteristics: You could try random forest classification based on the randomForest package or methods in stats::hclust. 3.5 PADi In GSClassifier, PADi is a pre-trained out-of-the-box model for GC personalized PAD subtypes calling. During the subtype calling, the gene rank relations based on individuals instead of the relative values across samples would be used. Thus, you don’t have to do batch normalization even though the data (the X input) come from multiple cohorts or platforms. More limitations were discussed in our paper that you had better know. In this section, we would show how to use PADi function series: PADi, callEnsemble, and parCallEnsemble functions. 3.5.1 Preparation of the test data X &lt;- testData$Kim2018_3 head(X) # PB-16-002 PB-16-003 PB-16-004 # ENSG00000121410 0.07075272 -2.08976724 -1.43569557 # ENSG00000148584 -1.49631022 -0.23917056 0.94827471 # ENSG00000175899 -0.77315329 0.52163146 0.91264015 # ENSG00000166535 -0.28860715 -0.45964255 -0.38401295 # ENSG00000256069 -0.25034243 0.06863867 0.14429081 # ENSG00000184389 0.08215945 -0.05966481 0.04937924 3.5.2 Use a specific function called PADi Very simple, just: res_padi &lt;- PADi(X = X, verbose = F) Check the result: head(res_padi) # SampleIDs BestCall BestCall_Max 1 2 3 4 # 1 PB-16-002 4 4 0.023372779 0.02631794 0.04864784 0.3336484 # 2 PB-16-003 4 4 0.007271698 0.08650742 0.01974812 0.9530730 # 3 PB-16-004 4 4 0.011559768 0.02922151 0.09018894 0.8649045 Actually, PADi is exactly based on a general function called callEnsemble. 3.5.3 Use the callEnsemble function Also simple, just: res_padi &lt;- callEnsemble( X = X, ens = NULL, geneAnnotation = NULL, geneSet = NULL, scaller = NULL, geneid = &quot;ensembl&quot;, subtype = &quot;PAD.train_20220916&quot;, verbose = F ) Check the result: head(res_padi) # SampleIDs BestCall BestCall_Max 1 2 3 4 # 1 PB-16-002 4 4 0.01338872 0.01624520 0.03965218 0.8052567 # 2 PB-16-003 4 4 0.04709511 0.08833681 0.03879361 0.6244038 # 3 PB-16-004 4 4 0.01389035 0.03638009 0.05852707 0.6980438 3.5.4 Parallel strategy for PADi Sometimes, the number of patients for subtype callings could be huge (hundreds or even tens of thousands). Thus, parallel computing (Windows or Linux pass; not tested in Mac or other OS) was also developed in the current version of the GSClassifier package. The parameter numCores was used to control the No. of CPU for computing (which depends on your CPU capacity). # No run for the tiny test data. With errors. # Method 1: res_padi &lt;- PADi(X = X, verbose = F, numCores = 4) # Method 2: res_padi &lt;- parCallEnsemble( X = X, ens = NULL, geneAnnotation = NULL, geneSet = NULL, scaller = NULL, geneids = &#39;ensembl&#39;, subtype = &#39;PAD.train_20220916&#39;, verbose = T, numCores = 4) 3.5.5 Single sample subtype calling In clinical practice, the single sample subtype calling might be one of the most common scenarios and is also supported by functions of the PADi series. Supposed that there is a GC patient, its information should be: X_ind &lt;- X[,1]; names(X_ind) &lt;- rownames(X) head(X_ind) # ENSG00000121410 ENSG00000148584 ENSG00000175899 ENSG00000166535 ENSG00000256069 # 0.07075272 -1.49631022 -0.77315329 -0.28860715 -0.25034243 # ENSG00000184389 # 0.08215945 Or it can also be another format: X_ind &lt;- as.matrix(X[,1]); rownames(X_ind) &lt;- rownames(X) head(X_ind) # [,1] # ENSG00000121410 0.07075272 # ENSG00000148584 -1.49631022 # ENSG00000175899 -0.77315329 # ENSG00000166535 -0.28860715 # ENSG00000256069 -0.25034243 # ENSG00000184389 0.08215945 Similar to multiples sample calling, just: check the result: head(res_padi) # SampleIDs BestCall BestCall_Max 1 2 3 4 # 1 target 4 4 0.02337278 0.02631794 0.04864784 0.3336484 Similarly, there is alternative choice: res_padi &lt;- callEnsemble( X = X_ind, ens = NULL, geneAnnotation = NULL, geneSet = NULL, scaller = NULL, geneid = &quot;ensembl&quot;, subtype = &quot;PAD.train_20220916&quot;, verbose = F ) Check the result: head(res_padi) # SampleIDs BestCall BestCall_Max 1 2 3 4 # 1 target 4 4 0.01338872 0.0162452 0.03965218 0.8052567 3.5.6 Of note In the results of PADi, two types of subtypes (BestCall and BestCall_Max) were integrated. BestCall was predicted via an xgboost model based on prior knowledge of PAD subtypes and the possibility matrix (columns 4 to 7 of four-subtype calling, for example), while BestCall_Max was predicted via maximum strategy. Empirically, BestCall seemed to be a better choice. PADi is individual-dependent and personalized, which means that the result of subtype calling would not be influenced by the data of others. 3.6 Use external models from luckyModel package In the future, there might be lots of models available as a resource of GSClassifier, such as luckyModel. Here we show how luckyModel support GSClassifier. First, install and load luckyModel: # Install luckyModel if (!requireNamespace(&quot;luckyModel&quot;, quietly = TRUE)) devtools::install_github(&quot;huangwb8/luckyModel&quot;) library(luckyModel) Check projects supported in current luckyModel: list_project() # [1] &quot;GSClassifier&quot; Check available models in the project: list_model(project=&#39;GSClassifier&#39;) # Available models in GSClassifier: # *Gibbs_PanCancerImmuneSubtype_v20190731 # *HWB_PAD_v20200110 Here, HWB_PAD_v20200110 is a standard name of PADi. They are the same. Taking PADi as an example, we here show how to use an external model from luckyModel. First, load a model: model &lt;- lucky_model(project = &#39;GSClassifier&#39;, developer=&#39;HWB&#39;, model = &#39;PAD&#39;, version = &#39;v20200110&#39;) Then, check the gene id type: model$geneSet # $PIAM # [1] &quot;ENSG00000122122&quot; &quot;ENSG00000117091&quot; &quot;ENSG00000163219&quot; &quot;ENSG00000136167&quot; # [5] &quot;ENSG00000005844&quot; &quot;ENSG00000123338&quot; &quot;ENSG00000102879&quot; &quot;ENSG00000010671&quot; # [9] &quot;ENSG00000185862&quot; &quot;ENSG00000104814&quot; &quot;ENSG00000134516&quot; &quot;ENSG00000100055&quot; # [13] &quot;ENSG00000082074&quot; &quot;ENSG00000113263&quot; &quot;ENSG00000153283&quot; &quot;ENSG00000198821&quot; # [17] &quot;ENSG00000185811&quot; &quot;ENSG00000117090&quot; &quot;ENSG00000171608&quot; # # $PIDG # [1] &quot;ENSG00000116667&quot; &quot;ENSG00000107771&quot; &quot;ENSG00000196782&quot; &quot;ENSG00000271447&quot; # [5] &quot;ENSG00000173517&quot; &quot;ENSG00000134686&quot; &quot;ENSG00000100614&quot; &quot;ENSG00000134247&quot; # [9] &quot;ENSG00000109686&quot; &quot;ENSG00000197321&quot; &quot;ENSG00000179981&quot; &quot;ENSG00000187189&quot; # [13] &quot;ENSG00000140836&quot; The model should use ensembl as the value of geneid parameter in callEnsemble series. Next, you can use the model like: res_padi &lt;- callEnsemble( X = X, ens = model$ens$Model, geneAnnotation = model$geneAnnotation, geneSet = model$geneSet, scaller = model$scaller$Model, geneid = &quot;ensembl&quot;, subtype = NULL, verbose = F ) Or just: res_padi &lt;- callEnsemble( X, ens = NULL, geneAnnotation = NULL, geneSet = NULL, scaller = NULL, geneid = &quot;ensembl&quot;, subtype = model, verbose = F ) They are exactly the same. Finally, check the result: head(res_padi) # SampleIDs BestCall BestCall_Max 1 2 3 4 # 1 PB-16-002 4 4 0.023372779 0.02631794 0.04864784 0.3336484 # 2 PB-16-003 4 4 0.007271698 0.08650742 0.01974812 0.9530730 # 3 PB-16-004 4 4 0.011559768 0.02922151 0.09018894 0.8649045 3.7 PanCancer immune subtypes GSClassifier could also call the PanCancer immune subtypes of Gibbs’ [2]. First, see data available in current GSClassifier: GSClassifier_Data() # Available data: # Usage example: # ImmuneSubtype.rds # PAD.train_20200110.rds # PAD.train_20220916.rds # PAD &lt;- readRDS(system.file(&quot;extdata&quot;, &quot;PAD.train_20200110.rds&quot;, package = &quot;GSClassifier&quot;)) # ImmuneSubtype &lt;- readRDS(system.file(&quot;extdata&quot;, &quot;ImmuneSubtype.rds&quot;, package = &quot;GSClassifier&quot;)) Let’s use our test data to do this: X &lt;- testData$Kim2018_3 symbol &lt;- convert(rownames(X)) rownames(X) &lt;- symbol X &lt;- X[!is.na(symbol),] dim(X) # [1] 19118 3 Have a check head(X) # PB-16-002 PB-16-003 PB-16-004 # A1BG 0.07075272 -2.08976724 -1.43569557 # A1CF -1.49631022 -0.23917056 0.94827471 # A2M -0.77315329 0.52163146 0.91264015 # A2ML1 -0.28860715 -0.45964255 -0.38401295 # RP11-118B22.6 -0.25034243 0.06863867 0.14429081 # A3GALT2 0.08215945 -0.05966481 0.04937924 PanCan Immune Subtype callings: res_pis &lt;- callEnsemble( X = X, ens = NULL, geneAnnotation = NULL, geneSet = NULL, scaller = NULL, geneid = &quot;symbol&quot;, subtype = &quot;ImmuneSubtype&quot;, verbose = F ) Check the result: head(res_pis) # SampleIDs BestCall BestCall_Max 1 2 3 # 1 PB-16-002 2 2 7.024834e-04 0.561300665 4.650267e-06 # 2 PB-16-003 4 4 6.600246e-07 0.018167170 1.677736e-04 # 3 PB-16-004 3 3 4.434882e-06 0.001126488 2.490904e-01 # 4 5 6 # 1 0.3171446472 0.009215117 0.002500323 # 2 0.5359434187 0.002049010 0.001899991 # 3 0.0001472232 0.006895948 0.006951177 Also, you can try to use luckyModel: pci &lt;- lucky_model( project = &quot;GSClassifier&quot;, model = &quot;PanCancerImmuneSubtype&quot;, developer = &quot;Gibbs&quot;, version = &quot;v20190731&quot; ) PanCan Immune Subtype callings: res_pis &lt;- callEnsemble( X = X, ens = NULL, geneAnnotation = NULL, geneSet = NULL, scaller = NULL, geneid = &quot;symbol&quot;, subtype = pci, verbose = F ) Finally, we take a look at the PanCancer immune subtypes model: ImmuneSubtype &lt;- readRDS(system.file(&quot;extdata&quot;, &quot;ImmuneSubtype.rds&quot;, package = &quot;GSClassifier&quot;)) names(ImmuneSubtype) # [1] &quot;ens&quot; &quot;scaller&quot; &quot;geneAnnotation&quot; &quot;geneSet&quot; Its gene annotation: head(ImmuneSubtype$geneAnnotation) # SYMBOL ENTREZID ENSEMBL # 235 ACTL6A 86 ENSG00000136518 # 294 ADAM9 8754 ENSG00000168615 # 305 ADAMTS1 9510 ENSG00000154734 # 322 ADAR 103 ENSG00000160710 # 340 ADCY7 113 ENSG00000121281 # 479 AIMP2 7965 ENSG00000106305 Its gene sets: ImmuneSubtype$geneSet # $LIexpression_score # [1] &quot;CCL5&quot; &quot;CD19&quot; &quot;CD37&quot; &quot;CD3D&quot; &quot;CD3E&quot; &quot;CD3G&quot; &quot;CD3Z&quot; &quot;CD79A&quot; &quot;CD79B&quot; # [10] &quot;CD8A&quot; &quot;CD8B1&quot; &quot;IGHG3&quot; &quot;IGJ&quot; &quot;IGLC1&quot; &quot;CD14&quot; &quot;LCK&quot; &quot;LTB&quot; &quot;MS4A1&quot; # # $CSF1_response # [1] &quot;CORO1A&quot; &quot;MNDA&quot; &quot;CCRL2&quot; &quot;SLC7A7&quot; &quot;HLA-DMA&quot; &quot;FYB&quot; # [7] &quot;RNASE6&quot; &quot;TLR2&quot; &quot;CTSC&quot; &quot;LILRB4&quot; &quot;PSCDBP&quot; &quot;CTSS&quot; # [13] &quot;RASSF4&quot; &quot;MSN&quot; &quot;CYBB&quot; &quot;LAPTM5&quot; &quot;DOCK2&quot; &quot;FCGR1A&quot; # [19] &quot;EVI2B&quot; &quot;ADCY7&quot; &quot;CD48&quot; &quot;ARHGAP15&quot; &quot;ARRB2&quot; &quot;SYK&quot; # [25] &quot;BTK&quot; &quot;TNFAIP3&quot; &quot;FCGR2A&quot; &quot;VSIG4&quot; &quot;FPRL2&quot; &quot;IL10RA&quot; # [31] &quot;IFI16&quot; &quot;ITGB2&quot; &quot;IL7R&quot; &quot;TBXAS1&quot; &quot;FMNL1&quot; &quot;FLI1&quot; # [37] &quot;RASSF2&quot; &quot;LYZ&quot; &quot;CD163&quot; &quot;CD97&quot; &quot;CCL2&quot; &quot;FCGR2B&quot; # [43] &quot;MERTK&quot; &quot;CD84&quot; &quot;CD53&quot; &quot;CD86&quot; &quot;HMHA1&quot; &quot;CTSL&quot; # [49] &quot;EVI2A&quot; &quot;TNFRSF1B&quot; &quot;CXCR4&quot; &quot;LCP1&quot; &quot;SAMHD1&quot; &quot;CPVL&quot; # [55] &quot;HLA-DRB1&quot; &quot;C13orf18&quot; &quot;GIMAP4&quot; &quot;SAMSN1&quot; &quot;PLCG2&quot; &quot;OSBPL3&quot; # [61] &quot;CD8A&quot; &quot;RUNX3&quot; &quot;FCGR3A&quot; &quot;AMPD3&quot; &quot;MYO1F&quot; &quot;CECR1&quot; # [67] &quot;LYN&quot; &quot;MPP1&quot; &quot;LRMP&quot; &quot;FGL2&quot; &quot;NCKAP1L&quot; &quot;HCLS1&quot; # [73] &quot;SELL&quot; &quot;CASP1&quot; &quot;SELPLG&quot; &quot;CD33&quot; &quot;GPNMB&quot; &quot;NCF2&quot; # [79] &quot;FNBP1&quot; &quot;IL18&quot; &quot;B2M&quot; &quot;SP140&quot; &quot;FCER1G&quot; &quot;LCP2&quot; # [85] &quot;LY86&quot; &quot;LAIR1&quot; &quot;IFI30&quot; &quot;TNFSF13B&quot; &quot;LST1&quot; &quot;FGR&quot; # [91] &quot;NPL&quot; &quot;PLEK&quot; &quot;CCL5&quot; &quot;PTPRC&quot; &quot;GNPTAB&quot; &quot;SLC1A3&quot; # [97] &quot;HCK&quot; &quot;NPC2&quot; &quot;C3AR1&quot; &quot;PIK3CG&quot; &quot;DAPK1&quot; &quot;ALOX5AP&quot; # [103] &quot;CSF1R&quot; &quot;CUGBP2&quot; &quot;APOE&quot; &quot;APOC1&quot; &quot;CD52&quot; &quot;LHFPL2&quot; # [109] &quot;C1orf54&quot; &quot;IKZF1&quot; &quot;SH2B3&quot; &quot;WIPF1&quot; # # $Module3_IFN_score # [1] &quot;IFI44&quot; &quot;IFI44L&quot; &quot;DDX58&quot; &quot;IFI6&quot; &quot;IFI27&quot; &quot;IFIT2&quot; &quot;IFIT1&quot; &quot;IFIT3&quot; # [9] &quot;CXCL10&quot; &quot;MX1&quot; &quot;OAS1&quot; &quot;OAS2&quot; &quot;OAS3&quot; &quot;HERC5&quot; &quot;SAMD9&quot; &quot;HERC6&quot; # [17] &quot;DDX60&quot; &quot;RTP4&quot; &quot;IFIH1&quot; &quot;STAT1&quot; &quot;TAP1&quot; &quot;OASL&quot; &quot;RSAD2&quot; &quot;ISG15&quot; # # $TGFB_score_21050467 # [1] &quot;MMP3&quot; &quot;MARCKSL1&quot; &quot;IGF2R&quot; &quot;LAMB1&quot; &quot;SPARC&quot; &quot;FN1&quot; # [7] &quot;ITGA4&quot; &quot;SMO&quot; &quot;MMP19&quot; &quot;ITGB8&quot; &quot;ITGA5&quot; &quot;NID1&quot; # [13] &quot;TIMP1&quot; &quot;SEMA3F&quot; &quot;RHOQ&quot; &quot;CTNNB1&quot; &quot;MMP2&quot; &quot;SERPINE1&quot; # [19] &quot;EPHB2&quot; &quot;COL16A1&quot; &quot;EPHA2&quot; &quot;TNC&quot; &quot;JUP&quot; &quot;ITGA3&quot; # [25] &quot;TCF7L2&quot; &quot;COL3A1&quot; &quot;CDH6&quot; &quot;WNT2B&quot; &quot;ADAM9&quot; &quot;DSP&quot; # [31] &quot;HSPG2&quot; &quot;ARHGAP1&quot; &quot;ITGB5&quot; &quot;IGFBP5&quot; &quot;ARHGDIA&quot; &quot;LRP1&quot; # [37] &quot;IGFBP2&quot; &quot;CTNNA1&quot; &quot;LRRC17&quot; &quot;MMP14&quot; &quot;NEO1&quot; &quot;EFNA5&quot; # [43] &quot;ITGB3&quot; &quot;EPHB3&quot; &quot;CD44&quot; &quot;IGFBP4&quot; &quot;TNFRSF1A&quot; &quot;RAC1&quot; # [49] &quot;PXN&quot; &quot;PLAT&quot; &quot;COL8A1&quot; &quot;WNT8B&quot; &quot;IGFBP3&quot; &quot;RHOA&quot; # [55] &quot;EPHB4&quot; &quot;MMP1&quot; &quot;PAK1&quot; &quot;MTA1&quot; &quot;THBS2&quot; &quot;CSPG2&quot; # [61] &quot;MMP17&quot; &quot;CD59&quot; &quot;DVL3&quot; &quot;RHOB&quot; &quot;COL6A3&quot; &quot;NOTCH2&quot; # [67] &quot;BSG&quot; &quot;MMP11&quot; &quot;COL1A2&quot; &quot;ZYX&quot; &quot;RND3&quot; &quot;THBS1&quot; # [73] &quot;RHOG&quot; &quot;ICAM1&quot; &quot;LAMA4&quot; &quot;DVL1&quot; &quot;PAK2&quot; &quot;ITGB2&quot; # [79] &quot;COL6A1&quot; &quot;FGD1&quot; # # $CHANG_CORE_SERUM_RESPONSE_UP # [1] &quot;CEP78&quot; &quot;LSM3&quot; &quot;LRRC40&quot; &quot;STK17A&quot; &quot;RPN1&quot; &quot;JUNB&quot; # [7] &quot;NUP85&quot; &quot;FLNC&quot; &quot;HMGN2&quot; &quot;RPP40&quot; &quot;UQCR10&quot; &quot;AIMP2&quot; # [13] &quot;CHEK1&quot; &quot;VTA1&quot; &quot;EXOSC8&quot; &quot;CENPO&quot; &quot;PNO1&quot; &quot;SLC16A1&quot; # [19] &quot;WDR77&quot; &quot;UBE2J1&quot; &quot;NOP16&quot; &quot;NUDT1&quot; &quot;SMC2&quot; &quot;SLC25A5&quot; # [25] &quot;NUPL1&quot; &quot;DLEU2&quot; &quot;PDAP1&quot; &quot;CCBL2&quot; &quot;COX17&quot; &quot;BCCIP&quot; # [31] &quot;PLG&quot; &quot;RGS8&quot; &quot;SNRPC&quot; &quot;PLK4&quot; &quot;NUTF2&quot; &quot;LSM4&quot; # [37] &quot;SMS&quot; &quot;EBNA1BP2&quot; &quot;C13orf27&quot; &quot;VDAC1&quot; &quot;PSMD14&quot; &quot;MYCBP&quot; # [43] &quot;SMURF2&quot; &quot;GNG11&quot; &quot;F3&quot; &quot;IL7R&quot; &quot;BRIP1&quot; &quot;HNRNPA2B1&quot; # [49] &quot;DCK&quot; &quot;ALKBH7&quot; &quot;HN1L&quot; &quot;MSN&quot; &quot;TPM1&quot; &quot;HYLS1&quot; # [55] &quot;HAUS1&quot; &quot;NUP93&quot; &quot;SNRPE&quot; &quot;ITGA6&quot; &quot;CENPN&quot; &quot;C11orf24&quot; # [61] &quot;GGH&quot; &quot;PFKP&quot; &quot;FARSA&quot; &quot;EIF2AK1&quot; &quot;CENPW&quot; &quot;TUBA4A&quot; # [67] &quot;TRA2B&quot; &quot;UMPS&quot; &quot;MRTO4&quot; &quot;NUDT15&quot; &quot;PGM2&quot; &quot;DBNDD1&quot; # [73] &quot;SNRPB&quot; &quot;MNAT1&quot; &quot;NUP35&quot; &quot;TCEB1&quot; &quot;HSPB11&quot; &quot;C19orf48&quot; # [79] &quot;ID3&quot; &quot;IPO4&quot; &quot;FARSB&quot; &quot;EIF4G1&quot; &quot;SKA1&quot; &quot;MFSD11&quot; # [85] &quot;PLAUR&quot; &quot;MARVELD2&quot; &quot;MCM3&quot; &quot;DHFR&quot; &quot;RNF41&quot; &quot;ID2&quot; # [91] &quot;H2AFZ&quot; &quot;CDK2&quot; &quot;NCLN&quot; &quot;ZWILCH&quot; &quot;DYNLT1&quot; &quot;C16orf61&quot; # [97] &quot;SLC25A40&quot; &quot;RHOC&quot; &quot;CCT5&quot; &quot;PDIA4&quot; &quot;SNRPA&quot; &quot;RBM14&quot; # [103] &quot;PDLIM7&quot; &quot;PITPNC1&quot; &quot;TPM3&quot; &quot;CORO1C&quot; &quot;ERLIN1&quot; &quot;PAICS&quot; # [109] &quot;TPRKB&quot; &quot;SKA2&quot; &quot;MYBL1&quot; &quot;SH3BP5L&quot; &quot;BRCA2&quot; &quot;SAR1A&quot; # [115] &quot;POLR3K&quot; &quot;MRPS28&quot; &quot;NUP107&quot; &quot;TUBG1&quot; &quot;PNN&quot; &quot;FAM167A&quot; # [121] &quot;RFC3&quot; &quot;MYL6&quot; &quot;MCM7&quot; &quot;MAGOHB&quot; &quot;FAM89B&quot; &quot;TOMM40&quot; # [127] &quot;CDCA4&quot; &quot;MT3&quot; &quot;MTHFD1&quot; &quot;PSMD12&quot; &quot;MYBL2&quot; &quot;CKLF&quot; # [133] &quot;NRIP3&quot; &quot;EZR&quot; &quot;C12orf24&quot; &quot;GPLD1&quot; &quot;SRM&quot; &quot;RAB3B&quot; # [139] &quot;NLN&quot; &quot;MT1F&quot; &quot;TNFRSF12A&quot; &quot;TPI1&quot; &quot;HAS2&quot; &quot;APOO&quot; # [145] &quot;FBXO41&quot; &quot;MRPL37&quot; &quot;GSTCD&quot; &quot;SDC1&quot; &quot;WDR54&quot; &quot;RNF138&quot; # [151] &quot;APITD1&quot; &quot;RMND5B&quot; &quot;ENO1&quot; &quot;MAP3K8&quot; &quot;TMEM130&quot; &quot;SNX17&quot; # [157] &quot;KRR1&quot; &quot;TAGLN&quot; &quot;PA2G4&quot; &quot;RUVBL1&quot; &quot;SNRPD1&quot; &quot;LOXL2&quot; # [163] &quot;POLE2&quot; &quot;MAPRE1&quot; &quot;IMP4&quot; &quot;EMP2&quot; &quot;PSMD2&quot; &quot;MET&quot; # [169] &quot;IFRD2&quot; &quot;LMNB2&quot; &quot;PLOD2&quot; &quot;NCEH1&quot; &quot;NME1&quot; &quot;STRA13&quot; # [175] &quot;ACTL6A&quot; &quot;DLEU1&quot; &quot;SNRPA1&quot; &quot;CBX1&quot; &quot;LYAR&quot; &quot;PTPLB&quot; # [181] &quot;PFN1&quot; &quot;CENPJ&quot; &quot;COTL1&quot; &quot;SPRYD7&quot; &quot;USPL1&quot; &quot;MRPL12&quot; # [187] &quot;ADAMTS1&quot; &quot;GLRX3&quot; &quot;WSB2&quot; &quot;MRPS16&quot; &quot;DCLRE1B&quot; &quot;MKKS&quot; # [193] &quot;C3orf26&quot; &quot;CPEB4&quot; &quot;SPAG17&quot; &quot;MLF1IP&quot; &quot;UAP1&quot; &quot;COQ2&quot; # [199] &quot;WDHD1&quot; &quot;DCBLD2&quot; &quot;KIAA0090&quot; &quot;SAR1B&quot; &quot;PSMA7&quot; &quot;PSMC3&quot; # [205] &quot;COPS6&quot; &quot;DUT&quot; &quot;PPIH&quot; &quot;PHF19&quot; &quot;TPM2&quot; &quot;MCTS1&quot; # [211] &quot;EIF4EBP1&quot; &quot;HNRNPR&quot; Enjoy GSClassifier! "],["model-establishment-via-gsclassifier.html", "Chapter 4 Model establishment via GSClassifier 4.1 About 4.2 Data preparation 4.3 Fitting models 4.4 Calling subtypes 4.5 Number of SubModel 4.6 Parameters for PADi training", " Chapter 4 Model establishment via GSClassifier 4.1 About Sometimes, researchers might have their gene signatures and know how many subtypes they want to call before (based on some knowledge). Gratifyingly, comprehensive functions were also provided in GSClassifier. In this section, we would show how to build a GSClassifier model like PADi. 4.2 Data preparation Load packages: library(GSClassifier) library(plyr) library(dplyr) Load data: # The test data is only for the demonstration of the modeling testData &lt;- readRDS(system.file(&quot;extdata&quot;, &quot;testData.rds&quot;, package = &quot;GSClassifier&quot;)) expr &lt;- testData$PanSTAD_expr_part design &lt;- testData$PanSTAD_phenotype_part Select training and testing cohorts across different platforms and PAD subtypes from PAD function: modelInfo &lt;- modelData( design, id.col = &quot;ID&quot;, variable = c(&quot;platform&quot;, &quot;PAD_subtype&quot;), Prop = 0.7, seed = 145 ) Check the result modelInfo: names(modelInfo) # [1] &quot;Repeat&quot; &quot;Data&quot; Explore the training cohort: head(modelInfo$Data$Train) # ID Dataset PAD_subtype PIAM_subtype PIDG_subtype platform # GSM1606509 GSM1606509 GSE65801 PAD-I high high GPL14550 # GSM1606517 GSM1606517 GSE65801 PAD-I high high GPL14550 # GSM1606503 GSM1606503 GSE65801 PAD-I high high GPL14550 # GSM1606525 GSM1606525 GSE65801 PAD-I high high GPL14550 # GSM1606511 GSM1606511 GSE65801 PAD-I high high GPL14550 # GSM1606527 GSM1606527 GSE65801 PAD-I high high GPL14550 Explore the internal validation cohort: head(modelInfo$Data$Valid) # ID Dataset PAD_subtype PIAM_subtype PIDG_subtype platform # GSM2235558 GSM2235558 GSE84437 PAD-I high high GPL6947 # GSM2235561 GSM2235561 GSE84437 PAD-II high low GPL6947 # GSM2235562 GSM2235562 GSE84437 PAD-IV low low GPL6947 # GSM2235563 GSM2235563 GSE84437 PAD-IV low low GPL6947 # GSM2235564 GSM2235564 GSE84437 PAD-IV low low GPL6947 # GSM2235567 GSM2235567 GSE84437 PAD-IV low low GPL6947 Get training data Xs and Ys: # Training data Xs &lt;- expr[,modelInfo$Data$Train$ID] y &lt;- modelInfo$Data$Train y &lt;- y[colnames(Xs),] Ys &lt;- ifelse(y$PAD_subtype == &#39;PAD-I&#39;,1,ifelse(y$PAD_subtype == &#39;PAD-II&#39;,2,ifelse(y$PAD_subtype == &#39;PAD-III&#39;,3,ifelse(y$PAD_subtype == &#39;PAD-IV&#39;,4,NA)))); table(Ys)/length(Ys) # Ys # 1 2 3 4 # 0.1010169 0.2474576 0.1694915 0.4820339 Get the number of subtype: # No. of subtypes nSubtype &lt;- length(unique(Ys)) print(nSubtype) # [1] 4 Also, you can take a look at the validation data: # Validating data Xs_valid &lt;- expr[,modelInfo$Data$Valid$ID] y &lt;- modelInfo$Data$Valid y &lt;- y[colnames(Xs_valid),] Ys_valid &lt;- ifelse(y$PAD_subtype == &#39;PAD-I&#39;,1,ifelse(y$PAD_subtype == &#39;PAD-II&#39;,2,ifelse(y$PAD_subtype == &#39;PAD-III&#39;,3,ifelse(y$PAD_subtype == &#39;PAD-IV&#39;,4,NA)))) table(Ys_valid)/length(Ys_valid) # Ys_valid # 1 2 3 4 # 0.09609121 0.24592834 0.16612378 0.49185668 Note: When you convert your phenotype into numeric, You CANNOT USE A ZERO VALUE, which is not supported by the xGboost. Other parameteres for modeling: # Build 20 models n=20 # In every model, 70% samples in the training cohort would be selected. sampSize=0.7 # Seed for sampling sampSeed = 2020 na.fill.seed = 2022 # A vector for approximate gene rank estimation breakVec=c(0, 0.25, 0.5, 0.75, 1.0) # Use 80% most variable gene &amp; gene-pairs for modeling ptail=0.8/2 # Automatical selection of parameters for xGboost auto = F if(!auto){ # Self-defined params. Fast. params = list(max_depth = 10, eta = 0.5, nrounds = 100, nthread = 10, nfold=5) caret.seed = NULL # No. of CPU for parallel computing. The optimized value depends on your CPU and RAM numCores = 4 } else { # caret::train strategy by GSClassifier:::cvFitOneModel2. Time consuming params = NULL caret.seed = 105 # Self-defined. For this exmaple training grid, there are 2×1×1×3×2×1×2=24 grids. Make sure that you have a computer with a powerfull CPU. grid = expand.grid( nrounds = c(100, 200), colsample_bytree = 1, min_child_weight = 1, eta = c(0.01, 0.1, 0.3), gamma = c(0.5, 0.3), subsample = 0.7, max_depth = c(5,8) ) # If you don&#39;t know how to set, just use the same number of your subtypes numCores = 4 } Finaly, you have to provide your gene sets as a list object: geneSet = &lt;Your gene sets&gt; Let’s take PAD as an example: PAD &lt;- readRDS(system.file(&quot;extdata&quot;, &quot;PAD.train_20220916.rds&quot;, package = &quot;GSClassifier&quot;)) geneSet &lt;- PAD$geneSet print(geneSet) # $PIAM # [1] &quot;ENSG00000122122&quot; &quot;ENSG00000117091&quot; &quot;ENSG00000163219&quot; &quot;ENSG00000136167&quot; # [5] &quot;ENSG00000005844&quot; &quot;ENSG00000123338&quot; &quot;ENSG00000102879&quot; &quot;ENSG00000010671&quot; # [9] &quot;ENSG00000185862&quot; &quot;ENSG00000104814&quot; &quot;ENSG00000134516&quot; &quot;ENSG00000100055&quot; # [13] &quot;ENSG00000082074&quot; &quot;ENSG00000113263&quot; &quot;ENSG00000153283&quot; &quot;ENSG00000198821&quot; # [17] &quot;ENSG00000185811&quot; &quot;ENSG00000117090&quot; &quot;ENSG00000171608&quot; # # $PIDG # [1] &quot;ENSG00000116667&quot; &quot;ENSG00000107771&quot; &quot;ENSG00000196782&quot; &quot;ENSG00000271447&quot; # [5] &quot;ENSG00000173517&quot; &quot;ENSG00000134686&quot; &quot;ENSG00000100614&quot; &quot;ENSG00000134247&quot; # [9] &quot;ENSG00000109686&quot; &quot;ENSG00000197321&quot; &quot;ENSG00000179981&quot; &quot;ENSG00000187189&quot; # [13] &quot;ENSG00000140836&quot; 4.3 Fitting models 4.3.1 GSClassifier model training Just fit the model like: if(!auto){ # Self-defined system.time( res &lt;- fitEnsembleModel(Xs, Ys, geneSet = geneSet, na.fill.method = c(&#39;quantile&#39;,&#39;rpart&#39;,NULL)[1], na.fill.seed = na.fill.seed, n = n, sampSize = sampSize, sampSeed = sampSeed , breakVec = breakVec, params = params, ptail = ptail, caret.grid = NULL, caret.seed = caret.seed, verbose = verbose, numCores = numCores) ) # user system elapsed # 0.08s 0.18s 92.70s } else { # caret::train-defined and time-consuming system.time( res &lt;- fitEnsembleModel(Xs, Ys, geneSet = geneSet, na.fill.method = c(&#39;quantile&#39;,&#39;rpart&#39;,NULL)[1], na.fill.seed = na.fill.seed, n = n, sampSize = sampSize, sampSeed = sampSeed , breakVec = breakVec, params = NULL, # This must be NULL ptail = ptail, caret.grid = grid, caret.seed = caret.seed, verbose = verbose, numCores = numCores) ) # user system elapsed # 1.10s 2.60s 2311.55s } # Remind me with a music when the process completed mymusic() You should save it for convenience: saveRDS(res,&#39;&lt;your path&gt;/train_ens.rds&#39;) Although an auto-parameter strategy (hyperparameter tuning) was provided inGSClassifier, it’s unknown whether this method could significantly improve your model performance. You can just try. It’s not a prior recommendation. Generally, setting auto=F in this script could be more cost-effective. Empirically, the speed of caret::train depends on the single-core performance of the CPU instead of the core number. 4.3.2 Scaller for the best call Next, we model the scaller for the training cohort, which would be used for BestCall based on the probability matrix in callEnsemble series. Here, scaller=NULL would cause an NA value of BestCall col. It’s not a big deal, because the probability matrix is the information we need. # Time-consuming modeling resTrain &lt;- parCallEnsemble(X = Xs, ens = res$Model, geneAnnotation = res$geneAnnotation, geneSet = geneSet, scaller = NULL, geneids = &quot;ensembl&quot;, subtype = NULL, numCores = numCores) # xgboost via best interation library(xgboost) dtrain &lt;- xgb.DMatrix(as.matrix(resTrain[4:(3 + nSubtype)]), label = Ys-1) cvRes &lt;- xgb.cv(data = dtrain, nrounds=100, nthread=10, nfold=5, max_depth=5, eta=0.5, early_stopping_rounds=100, num_class = 4, objective = &quot;multi:softmax&quot;) # xgboost via best interation bst &lt;- xgboost(data = dtrain, max_depth=5, eta=0.5, nrounds = cvRes$best_iteration, nthread=10, num_class = 4, objective = &quot;multi:softmax&quot;) Ys_pred &lt;- predict(bst, as.matrix(resTrain[4:7])) + 1 mean(Ys_pred == Ys) # Prediction rates # Ensemble results scaller.train &lt;- list( Repeat = list( data = dtrain, max_depth=5, eta=0.5, nrounds = cvRes$best_iteration, nthread=10, num_class = 4, objective = &quot;multi:softmax&quot; ), Model = bst ) 4.3.3 Assemble your model For more information of geneAnnotation, you could see the Suggestions for GSClassifier model developers: Gene Annotation section for assistance. Here we give an example: l.train &lt;- list() # bootstrap models based on the training cohort l.train[[&#39;ens&#39;]] &lt;- res # Scaller model l.train[[&#39;scaller&#39;]] &lt;- scaller.train # a data frame contarining gene annotation for IDs convertion l.train[[&#39;geneAnnotation&#39;]] &lt;- &lt;Your gene annotation&gt; # Your gene sets l.train[[&#39;geneSet&#39;]] &lt;- geneSet Finally, save it for downstream analysis saveRDS(l.train,&#39;&lt;Your path&gt;/train.rds&#39;) About model constributions, you can go Advanced development in here or here for more information. 4.3.4 Of note You can take a look at the PAD.train_20220916 model (PADi). You have to make your model frame similar to the PAD.train_20220916 model. l.train &lt;- readRDS(system.file(&quot;extdata&quot;, &quot;PAD.train_20220916.rds&quot;, package = &quot;GSClassifier&quot;)) names(l.train) # [1] &quot;ens&quot; &quot;scaller&quot; &quot;geneAnnotation&quot; &quot;geneSet&quot; The time of GSClassifier modeling depends on the number of individual models (controlled by n)/called subtypes/gene signatures, automatic parameter selection, and your CPU capacity. 4.4 Calling subtypes Supposed that you had got a GSClassifier model, next you want to use it for personalized subtype calling. Just: # Load your model l &lt;- readRDS(&#39;&lt;Your path&gt;/train.rds&#39;) # subtype calling res_i = callEnsemble( X, ens = l$ens$Model, geneAnnotation = l$geneAnnotation, geneSet = l$geneSet, scaller = l$scaller$Model, geneid = &lt;ID type of your training data&gt;, subtype = NULL, verbose = T ) The usage of parCallEnsemble (for a huge amount of samples) is similar. Empirically, parCallEnsemble can not perform better than callEnsemble in a small cohort for the process of xgboost would take advantage of multiple CPU cores. 4.5 Number of SubModel In the latest version of PADi, we trained 100 SubModels for subtype calling. We also trained some models with different training cohorts (200 models, Figure 4.1) or with different numbers (20, 50, 100, 200, 500 and 1000 for the same training cohort, Figure 4.2) of sub-models. Our results showed that the performance of these models is similar in the “Kim2018” cohort. Figure 4.1: Performance of PADi models in “Kim2018” cohort with 200 different training seeds. Each row is the data of a seed. Each column is a sample from the “Kim2018” cohort. Figure 4.2: Performance of PADi models in “Kim2018” cohort with different numbers of SubModel. The x-axis is the number of SubModels, and the y-axis is the AUC of ROC analysis for ICI response prediction. 4.6 Parameters for PADi training Here’re some key parameters about how the latest PADi (PAD.train_v20220916) was trained. # Build 100 models n = 100 # In every model, 70% samples in the training cohort would be selected. sampSize = 0.7 # Seed for sampling sampSeed = 2020 na.fill.seed = 443 # A vector for gene rank estimation breakVec = c(0, 0.25, 0.5, 0.75, 1.0) # Use 80% most variable gene &amp; gene-pairs for modeling ptail = 0.8/2 # Automatical selection of parameters for xGboost # self-defined params. Fast. params = list(max_depth = 10, eta = 0.5, nrounds = 100, nthread = 10, nfold=5) caret.seed = NULL Enjoy GSClassifier! "],["suggestions-for-gsclassifier-model-developers.html", "Chapter 5 Suggestions for GSClassifier model developers 5.1 About 5.2 Available models 5.3 Components of a GSClassifier model 5.4 Submit models to luckyModel package 5.5 Repeatablility of models 5.6 Gene Annotation", " Chapter 5 Suggestions for GSClassifier model developers 5.1 About The book R packages is a straightaway and useful reference book for R developers. The free-access website for R packages is https://r-pkgs.org/. As a developer of R, if you haven’t heard about it, it’s strongly recommended to just read it. Hadley Wickham, the main author of the book, is an active R developer and has led some masterworks like ggplot2 and plyr. With GSClassifier package, it could be easy for users to build a model only with certain gene sets and transcriptomics data. If you are interested in sharing your model, GSClassifier also provides a simple methodology for this vision. In this section, let’s see how to achieve it! First, load the package library(GSClassifier) # Loading required package: luckyBase 5.2 Available models With GSClassifier_Data(), all models supported in the current GSClassifier package would be shown. GSClassifier_Data() # Available data: # Usage example: # ImmuneSubtype.rds # PAD.train_20200110.rds # PAD.train_20220916.rds # PAD &lt;- readRDS(system.file(&quot;extdata&quot;, &quot;PAD.train_20200110.rds&quot;, package = &quot;GSClassifier&quot;)) # ImmuneSubtype &lt;- readRDS(system.file(&quot;extdata&quot;, &quot;ImmuneSubtype.rds&quot;, package = &quot;GSClassifier&quot;)) For more details of GSClassifier_Data(), just: ?GSClassifier_Data() Set model=F, all .rds data would be showed: GSClassifier_Data(model = F) # Available data: # Usage example: # general-gene-annotation.rds # ImmuneSubtype.rds # PAD.train_20200110.rds # PAD.train_20220916.rds # testData.rds # PAD &lt;- readRDS(system.file(&quot;extdata&quot;, &quot;PAD.train_20200110.rds&quot;, package = &quot;GSClassifier&quot;)) # ImmuneSubtype &lt;- readRDS(system.file(&quot;extdata&quot;, &quot;ImmuneSubtype.rds&quot;, package = &quot;GSClassifier&quot;)) 5.3 Components of a GSClassifier model Currently, a GSClassifier model and related product environments are designed as a list object. Let’s take PAD.train_20210110(also called PADi) as an example. PADi &lt;- readRDS(system.file(&quot;extdata&quot;, &quot;PAD.train_20220916.rds&quot;, package = &quot;GSClassifier&quot;)) This picture shows the components of PADi: Figure 5.1: Details of a GSClassifier model As shown, a typical GSClassifier model is consist of four parts (with different colors in the picture): 1. ens: Repeat: productive parameters of GSClassifier models Model: GSClassifier models. Here, PADi had 20 models from different subs of the training cohorts 2. scaller: Repeat: productive parameters of the scaller model, which was used for BestCall calling Model: the scaller model 3. geneAnnotation: a data frame containing gene annotation information 4. geneSet: a list contains several gene sets Thus, you can assemble your model like: model &lt;- list() # bootstrap models based on the training cohort model[[&#39;ens&#39;]] &lt;- &lt;Your model for subtypes calling&gt; # Scaller model model[[&#39;scaller&#39;]] &lt;- &lt;Your scaller for BestCall calling&gt; # a data frame contarining gene annotation for IDs convertion model[[&#39;geneAnnotation&#39;]] &lt;- &lt;Your gene annotation&gt; # Your gene sets model[[&#39;geneSet&#39;]] &lt;- &lt;Your gene sets&gt; saveRDS(model, &#39;your-model.rds&#39;) More tutorials for model establishment, please go to markdown tutorial or html tutorial. 5.4 Submit models to luckyModel package Considering most users of GSClassifier might not need lots of models, We divided the model storage feature into a new ensemble package called luckyModel. Don’t worry, the usage is very easy! If you want to submit your model, you should apply for a contributor of luckyModel first. Then, just send the model (.rds) into the inst/extdata/&lt;project&gt; path of luckyModel. After an audit, your branch would be accepted and available for the users. The name of your model must be the format as follows: # &lt;project&gt; GSClassifier # &lt;creator&gt;_&lt;model&gt;_v&lt;yyyymmdd&gt;: HWB_PAD_v20211201.rds 5.5 Repeatablility of models For repeatability, you had better submit a .zip or .tar.gz file containing the information of your model. Here are some suggestions: &lt;creator&gt;_&lt;model&gt;_v&lt;yyyymmdd&gt;.md Destinations: Why you develop the model Design: The evidence for gene sigatures, et al Data sources: The data for model training and validating, et al Applications: Where to use your model Limintations: Limitation or improvement direction of your model &lt;creator&gt;_&lt;model&gt;_v&lt;yyyymmdd&gt;.R: The code you used for model training and validating. Data-of-&lt;creator&gt;_&lt;model&gt;_v&lt;yyyymmdd&gt;.rds (Optional): Due to huge size of omics data, it’s OK for you not to submit the raw data. Welcome your contributions! 5.6 Gene Annotation For convenience, we provided a general gene annotation dataset for different genomics: gga &lt;- readRDS(system.file(&quot;extdata&quot;, &quot;general-gene-annotation.rds&quot;, package = &quot;GSClassifier&quot;)) names(gga) # [1] &quot;hg38&quot; &quot;hg19&quot; &quot;mm10&quot; I believe they’re enough for routine medicine studies. Here, take a look at hg38: hg38 &lt;- gga$hg38 head(hg38) # ENSEMBL SYMBOL ENTREZID # 1 ENSG00000223972 DDX11L1 100287102 # 3 ENSG00000227232 WASH7P &lt;NA&gt; # 4 ENSG00000278267 MIR6859-1 102466751 # 5 ENSG00000243485 RP11-34P13.3 &lt;NA&gt; # 6 ENSG00000284332 MIR1302-2 100302278 # 7 ENSG00000237613 FAM138A 645520 With this kind of data, it’s simple to customize your own gene annotation (take PADi as examples): tGene &lt;- as.character(unlist(PADi$geneSet)) geneAnnotation &lt;- hg38[hg38$ENSEMBL %in% tGene, ] dim(geneAnnotation) # [1] 32 3 Have a check: head(geneAnnotation) # ENSEMBL SYMBOL ENTREZID # 353 ENSG00000171608 PIK3CD 5293 # 1169 ENSG00000134686 PHC2 1912 # 2892 ENSG00000134247 PTGFRN 5738 # 3855 ENSG00000117090 SLAMF1 6504 # 3858 ENSG00000117091 CD48 962 # 4043 ENSG00000198821 CD247 919 This geneAnnotation could be the model[['geneAnnotation']]. Also, we use a function called convert to do gene ID convertion. luckyBase::convert(c(&#39;GAPDH&#39;,&#39;TP53&#39;), &#39;SYMBOL&#39;, &#39;ENSEMBL&#39;, hg38) # [1] &quot;ENSG00000111640&quot; &quot;ENSG00000141510&quot; Note: the luckyBase package integrates lots of useful tiny functions, you could explore it sometimes. "],["references.html", "References", " References 1. Thorsson V, Gibbs DL, Brown SD, et al. The immune landscape of cancer. Immunity 2018; 48:812–830 e14 2. Gibbs DL. Robust classification of immune subtypes in cancer. 2020; 3. Chen T, He T, Benesty M, et al. Xgboost: Extreme gradient boosting. 2015; 1:1–4 4. Geman D, d’Avignon C, Naiman DQ, et al. Classifying gene expression profiles from pairwise mRNA comparisons. Stat Appl Genet Mol Biol 2004; 3:Article19 5. Zhao H, Logothetis CJ, Gorlov IP. Usefulness of the top-scoring pairs of genes for prediction of prostate cancer progression. Prostate Cancer Prostatic Dis 2010; 13:252–9 6. Youssef YM, White NM, Grigull J, et al. Accurate molecular classification of kidney cancer subtypes using microRNA signature. Eur Urol 2011; 59:721–30 7. Auslander N, Zhang G, Lee JS, et al. Robust prediction of response to immune checkpoint blockade therapy in metastatic melanoma. Nat Med 2018; 24:1545–1549 8. Troyanskaya O, Cantor M, Sherlock G, et al. Missing value estimation methods for DNA microarrays. Bioinformatics 2001; 17:520–5 9. Arbeitman MN, Furlong EE, Imam F, et al. Gene expression during the life cycle of drosophila melanogaster. Science 2002; 297:2270–5 10. Zhu X, Wang J, Sun B, et al. An efficient ensemble method for missing value imputation in microarray gene expression data. BMC Bioinformatics 2021; 22:188 11. Lin W-C, Tsai C-F. Missing value imputation: A review and analysis of the literature (2006–2017). 2020; 53:1487–1509 12. Hasan MK, Alam MA, Roy S, et al. Missing value imputation affects the performance of machine learning: A review and analysis of the literature (2010–2021). Informatics in Medicine Unlocked 2021; 27:100799 13. Wang A, Yang J, An N. Regularized sparse modelling for microarray missing value estimation. 2021; 9:16899–16913 14. Chen T, He T, Benesty M, et al. Xgboost: Extreme gradient boosting. 2022; 15. Robin X, Turck N, Hainard A, et al. pROC: An open-source package for r and s+ to analyze and compare ROC curves. BMC Bioinformatics 2011; 12:77 16. Zhu S, Kong W, Zhu J, et al. The genetic algorithm-aided three-stage ensemble learning method identified a robust survival risk score in patients with glioma. Brief Bioinform 2022; 17. Tong M, Zheng W, Li H, et al. Multi-omics landscapes of colorectal cancer subtypes discriminated by an individualized prognostic signature for 5-fluorouracil-based chemotherapy. Oncogenesis 2016; 5:e242 18. Qi L, Li Y, Qin Y, et al. An individualised signature for predicting response with concordant survival benefit for lung adenocarcinoma patients receiving platinum-based chemotherapy. Br J Cancer 2016; 115:1513–1519 19. Huang H, Zou Y, Zhang H, et al. A qualitative transcriptional prognostic signature for patients with stage i-II pancreatic ductal adenocarcinoma. Transl Res 2020; 219:30–44 20. Zheng H, Song K, Fu Y, et al. An absolute human stemness index associated with oncogenic dedifferentiation. Brief Bioinform 2021; 22:2151–2160 21. Kong W, He L, Zhu J, et al. An immunity and pyroptosis gene-pair signature predicts overall survival in acute myeloid leukemia. Leukemia 2022; 22. Liu K, Geng Y, Wang L, et al. Systematic exploration of the underlying mechanism of gemcitabine resistance in pancreatic adenocarcinoma. Mol Oncol 2022; 16:3034–3051 23. Zheng H, Xie J, Song K, et al. StemSC: A cross-dataset human stemness index for single-cell samples. Stem Cell Res Ther 2022; 13:115 24. Monti S, Tamayo P, Mesirov J, et al. Consensus clustering: A resampling-based method for class discovery and visualization of gene expression microarray data. 2003; 52:91–118 25. Wilkerson MD, Hayes DN. ConsensusClusterPlus: A class discovery tool with confidence assessments and item tracking. 2010; 26:1572–1573 26. Hayes DN, Monti S, Parmigiani G, et al. Gene expression profiling reveals reproducible human lung adenocarcinoma subtypes in multiple independent patient cohorts. 2006; 24:5079–5090 27. Verhaak RG, Hoadley KA, Purdom E, et al. Integrated genomic analysis identifies clinically relevant subtypes of glioblastoma characterized by abnormalities in PDGFRA, IDH1, EGFR, and NF1. 2010; 17:98–110 "]]
